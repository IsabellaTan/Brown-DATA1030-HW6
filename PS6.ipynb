{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891ee364",
   "metadata": {},
   "source": [
    "## Problem set 6\n",
    "\n",
    "## Name: [Yawen Tan]\n",
    "\n",
    "## Link to your PS6 github repo: [https://github.com/IsabellaTan/Brown-DATA1030-HW6]\n",
    "\n",
    "### Problem 0 \n",
    "\n",
    "-2 points for every missing green OK sign. \n",
    "\n",
    "Make sure you are in the DATA1030 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf77c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.10\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 2.2.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.10.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.6.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.2.3 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m xgboost version 3.0.0 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m shap version 0.47.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m polars version 1.27.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m seaborn version 0.13.2 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.10 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.10\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.10\"):\n",
    "    print(FAIL, \"Python version 3.12.10 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'numpy': \"2.2.5\", 'matplotlib': \"3.10.1\",'sklearn': \"1.6.1\", \n",
    "                'pandas': \"2.2.3\",'xgboost': \"3.0.0\", 'shap': \"0.47.2\", \n",
    "                'polars': \"1.27.1\", 'seaborn': \"0.13.2\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570da73-48b5-4cf5-9265-36619f375075",
   "metadata": {},
   "source": [
    "## Problem 1 (5 points)\n",
    "\n",
    "Write a function called `linear_ML_pipeline` which takes training, validation, test sets, and a boolean variable called `is_classif` as input. The variable `is_classif` is `True` if the target variable is categorial (classification problem), and `False` if the target variable is continuous (regression problem). The function also takes the following lists as inputs:\n",
    "- continuous_ftrs: the column names of continuous features\n",
    "- ordinal_ftrs: the column names of ordinal features\n",
    "- ordinal_cats: the ordered list of categories for each ordinal feature\n",
    "- categorical_ftrs: the column names of categorical features\n",
    "\n",
    "Within the function, perform the following steps:\n",
    "- write the docstring\n",
    "- test the inputs! write at least 10 tests. make sure that among other things, all features are accounted for in the lists.\n",
    "- preprocess the sets using sklearn and make sure to fit_transform the training set, and transform the validation and test sets\n",
    "- fit a logistic regression model if `is_classif` is `True`, a linear regression model otherwise\n",
    "- use the elastic net regularization and tune both hyperparameters (alpha or C and l1_ratio)\n",
    "- perform CV and select the hyperparameter combo which optimizes the validation score\n",
    "- calculate the test score\n",
    "- your function should return the best model, its hyperparameters, and the test score\n",
    "\n",
    "You will use this function to solve problems 2 and 3. Data splitting will be performed before the function is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4436f7c-0221-4456-81bb-2a2b5f47de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n",
    "\n",
    "def linear_ML_pipeline(X_train, y_train, X_val, y_val, X_test, y_test, is_classif, continuous_ftrs, ordinal_ftrs, ordinal_cats, categorical_ftrs, random_state=42):\n",
    "\n",
    "    '''\n",
    "    Build and evaluate a complete linear-model pipeline that supports both\n",
    "    classification (Logistic Regression) and regression (Elastic Net),\n",
    "    with preprocessing, elastic-net regularization, manual hyperparameter tuning\n",
    "    over a grid, validation-based model selection, and final test evaluation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas.DataFrame\n",
    "        Feature matrix for the training split. Columns must match X_val and X_test\n",
    "        exactly (same names and order).\n",
    "    y_train : pandas.Series or 1D array-like\n",
    "        Target vector for the training split.\n",
    "    X_val : pandas.DataFrame\n",
    "        Feature matrix for the validation split (used to select hyperparameters).\n",
    "    y_val : pandas.Series or 1D array-like\n",
    "        Target vector for the validation split.\n",
    "    X_test : pandas.DataFrame\n",
    "        Feature matrix for the test split (held out for final evaluation only).\n",
    "    y_test : pandas.Series or 1D array-like\n",
    "        Target vector for the test split.\n",
    "    is_classif : bool\n",
    "        If True, fit a Logistic Regression model (elastic net penalty) for a\n",
    "        classification task; if False, fit an ElasticNet regressor for a\n",
    "        regression task.\n",
    "    continuous_ftrs : list[str]\n",
    "        Column names (in X_*) that are continuous features to be standardized\n",
    "        (mean=0, std=1).\n",
    "    ordinal_ftrs : list[str]\n",
    "        Column names (in X_*) that are ordinal categorical features to be\n",
    "        encoded via OrdinalEncoder using the specified order in `ordinal_cats`.\n",
    "    ordinal_cats : list[list[Any]]\n",
    "        Ordered categories for each ordinal feature. The i-th list provides the\n",
    "        category order for ordinal_ftrs[i].\n",
    "    categorical_ftrs : list[str]\n",
    "        Column names (in X_*) that are nominal (unordered) categorical features\n",
    "        to be encoded via OneHotEncoder(handle_unknown='ignore').\n",
    "    random_state : int, default=42\n",
    "        Random seed used where applicable (e.g., solvers), to ensure reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    best_model : sklearn estimator (already fitted)\n",
    "        The model refit on TRAIN+VAL using the best hyperparameters discovered\n",
    "        on the validation set (classification: LogisticRegression with penalty='elasticnet'\n",
    "        and solver='saga'; regression: ElasticNet).\n",
    "    best_hyperparams : dict\n",
    "        The best hyperparameter combination found on the validation set. Includes\n",
    "        at least {'l1_ratio': ..., 'C': ...} for classification or\n",
    "        {'l1_ratio': ..., 'alpha': ...} for regression, and any other key settings\n",
    "        needed to reproduce the best model.\n",
    "    test_score : float\n",
    "        Final performance on the held-out test set. For classification, accuracy;\n",
    "        for regression, R^2. (These defaults follow the introductory course\n",
    "        conventions.)\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> # Suppose you already split your data and defined feature groups:\n",
    "    >>> best_model, best_params, test_score = linear_ML_pipeline(\n",
    "    ...     X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    ...     is_classif=True,\n",
    "    ...     continuous_ftrs=['age','capital_gain','capital_loss','hours_per_week'],\n",
    "    ...     ordinal_ftrs=['education'],\n",
    "    ...     ordinal_cats=[[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',\n",
    "    ...                    ' 10th',' 11th',' 12th',' HS-grad',' Some-college',\n",
    "    ...                    ' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',\n",
    "    ...                    ' Prof-school',' Doctorate']],\n",
    "    ...     categorical_ftrs=['workclass','marital-status','occupation',\n",
    "    ...                       'relationship','race','sex','native-country'],\n",
    "    ...     random_state=42\n",
    "    ... )\n",
    "    >>> best_params\n",
    "    {'penalty': 'elasticnet', 'solver': 'saga', 'C': 1.0, 'l1_ratio': 0.5, 'max_iter': 10000}\n",
    "    >>> test_score\n",
    "    0.85  # accuracy on the held-out test set (example)\n",
    "    '''\n",
    "    # import library\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "    from sklearn.model_selection import ParameterGrid\n",
    "    from sklearn.metrics import accuracy_score, r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "    # TEST\n",
    "    #----------------------------------------------------------------\n",
    "    # Test 1: if X_train, X_val, X_test are not pandas DataFrames, raise ValueError\n",
    "    if not all(isinstance(x, pd.DataFrame) for x in [X_train, X_val, X_test]):\n",
    "        raise ValueError(\"X_train, X_val, and X_test must all be pandas DataFrames\")\n",
    "\n",
    "    # Test 2: if y_train, y_val, y_test are not pandas Series, raise ValueError\n",
    "    if not all(isinstance(y, (pd.Series, np.ndarray, list)) for y in [y_train, y_val, y_test]):\n",
    "        raise ValueError(\"y_train, y_val, and y_test must each be a pandas Series, numpy array, or list\")\n",
    "\n",
    "    # Test 3: if any of the feature DataFrames are empty (0 rows or 0 columns), raise ValueError\n",
    "    for name, df in zip([\"X_train\", \"X_val\", \"X_test\"], [X_train, X_val, X_test]):\n",
    "        if df.shape[0] == 0:\n",
    "            raise ValueError(f\"{name} has 0 rows\")\n",
    "        if df.shape[1] == 0:\n",
    "            raise ValueError(f\"{name} has 0 columns\")\n",
    "\n",
    "    # Test 4: ensure column names and order match across X_train, X_val, X_test\n",
    "    if list(X_train.columns) != list(X_val.columns) or list(X_train.columns) != list(X_test.columns):\n",
    "        raise ValueError(\"Column names and order must be identical across X_train, X_val, and X_test\")\n",
    "\n",
    "    # Test 5: ensure feature lists are all Python lists\n",
    "    for var_name, var in {\n",
    "        \"continuous_ftrs\": continuous_ftrs,\n",
    "        \"ordinal_ftrs\": ordinal_ftrs,\n",
    "        \"ordinal_cats\": ordinal_cats,\n",
    "        \"categorical_ftrs\": categorical_ftrs,\n",
    "    }.items():\n",
    "        if not isinstance(var, list):\n",
    "            raise ValueError(f\"{var_name} must be provided as a list\")\n",
    "\n",
    "    # Test 6: check that every feature column is accounted for exactly once\n",
    "    all_features = continuous_ftrs + ordinal_ftrs + categorical_ftrs\n",
    "    if set(all_features) != set(X_train.columns):\n",
    "        missing = set(X_train.columns) - set(all_features)\n",
    "        extra = set(all_features) - set(X_train.columns)\n",
    "        raise ValueError(f\"Feature mismatch: missing {missing}, extra {extra}\")\n",
    "\n",
    "    # Test 7: check for overlaps among feature groups\n",
    "    if (set(continuous_ftrs) & set(ordinal_ftrs)) or \\\n",
    "       (set(continuous_ftrs) & set(categorical_ftrs)) or \\\n",
    "       (set(ordinal_ftrs) & set(categorical_ftrs)):\n",
    "        raise ValueError(\"continuous_ftrs, ordinal_ftrs, and categorical_ftrs must not overlap\")\n",
    "\n",
    "    # Test 8: ensure ordinal_ftrs and ordinal_cats have the same length\n",
    "    if len(ordinal_ftrs) != len(ordinal_cats):\n",
    "        raise ValueError(\"Length of ordinal_cats must match the number of ordinal_ftrs\")\n",
    "\n",
    "    # Test 9: ensure continuous_ftrs is not empty\n",
    "    if continuous_ftrs == [] and ordinal_ftrs == [] and categorical_ftrs == []:\n",
    "        raise ValueError(\"At least one feature list must be non-empty\")\n",
    "\n",
    "    # Test 10: ensure is_classif is a boolean\n",
    "    if not isinstance(is_classif, bool):\n",
    "        raise ValueError(\"is_classif must be a boolean value (True for classification, False for regression)\")\n",
    "\n",
    "    # Test 11: ensure target lengths match feature lengths\n",
    "    for (x, y, name) in [(X_train, y_train, \"train\"), (X_val, y_val, \"validation\"), (X_test, y_test, \"test\")]:\n",
    "        if len(x) != len(y):\n",
    "            raise ValueError(f\"Length mismatch between X_{name} and y_{name}\")\n",
    "\n",
    "    # Test 12: optional—check for duplicate columns in X_train\n",
    "    if X_train.columns.duplicated().any():\n",
    "        raise ValueError(\"Duplicate column names detected in X_train\")\n",
    "\n",
    "\n",
    "    # Data Preprocessing\n",
    "    #----------------------------------------------------------------\n",
    "    # Continuous features: StandardScaler\n",
    "    continuous_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "    # Ordinal features: OrdinalEncoder\n",
    "    ordinal_transformer = Pipeline(steps=[('encoder', OrdinalEncoder(categories=ordinal_cats))])\n",
    "\n",
    "    # Nominal (categorical) features: OneHotEncoder\n",
    "    categorical_transformer = Pipeline(steps=[('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "    # Combine all three transformers using ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cont', continuous_transformer, continuous_ftrs),\n",
    "            ('ord', ordinal_transformer, ordinal_ftrs),\n",
    "            ('cat', categorical_transformer, categorical_ftrs)],\n",
    "        remainder='drop'  # if feature list cover all the column, drop left feature\n",
    "        )\n",
    "\n",
    "    # Fit on training data, transform all sets\n",
    "    X_train_prep = preprocessor.fit_transform(X_train)\n",
    "    X_val_prep = preprocessor.transform(X_val)\n",
    "    X_test_prep = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    #  Model setup and Elastic Net regularization\n",
    "    #----------------------------------------------------------------\n",
    "\n",
    "    if is_classif:\n",
    "        # Logistic Regression + Elastic Net Regularization\n",
    "        base_model = LogisticRegression(\n",
    "            penalty=\"elasticnet\",\n",
    "            solver=\"saga\",\n",
    "            max_iter=5000,\n",
    "            random_state=random_state)\n",
    "\n",
    "        # Parameter tuning\n",
    "        param_grid = {\n",
    "            \"C\": np.logspace(-2, 1, 2),       # [1e-2, 1, 1e2]\n",
    "            \"l1_ratio\": np.linspace(0.1, 1.0, 5)}\n",
    "        score_func = accuracy_score\n",
    "\n",
    "    else:\n",
    "        # Linear Regression：ElasticNet\n",
    "        base_model = ElasticNet(\n",
    "            max_iter=5000,\n",
    "            random_state=random_state)\n",
    "\n",
    "        # Parameter tuning\n",
    "        param_grid = {\n",
    "            \"alpha\": np.logspace(-2, 1, 2),   # [1e-2, 1, 1e2]\n",
    "            \"l1_ratio\": np.linspace(0.1, 1.0, 5)}\n",
    "\n",
    "        score_func = r2_score # use R square\n",
    "\n",
    "\n",
    "    # Cross-validation, hyperparameter tuning, and model selection\n",
    "    # ----------------------------------------------------------------\n",
    "    best_val_score = -np.inf  # record the best score for validation set\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    # set a list to store result\n",
    "    results = []\n",
    "\n",
    "    # Loop every combinition of parameter\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        # Create new model for new parameter\n",
    "        model = base_model.set_params(**params)\n",
    "        # Fit the train data\n",
    "        model.fit(X_train_prep, y_train)\n",
    "        # Predit on train and validation data\n",
    "        y_train_pred = model.predict(X_train_prep)\n",
    "        y_val_pred = model.predict(X_val_prep)\n",
    "        # Calcuate the score\n",
    "        train_score = score_func(y_train, y_train_pred)\n",
    "        val_score = score_func(y_val, y_val_pred)\n",
    "        mse_val = None\n",
    "        if not is_classif:\n",
    "            mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "        # Store the result\n",
    "        results.append({\n",
    "            **params,\n",
    "            \"train_score\": train_score,\n",
    "            \"val_score\": val_score,\n",
    "            \"mse_val\": mse_val\n",
    "        })\n",
    "        # Update best model\n",
    "        if val_score > best_val_score:\n",
    "            best_val_score = val_score\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "\n",
    "    # Refit best model on TRAIN+VAL, evaluate on TEST\n",
    "    # ---------------------------------------------------------\n",
    "    # Combine train and validation set\n",
    "    X_trainval = np.vstack([X_train_prep, X_val_prep])\n",
    "    y_trainval = np.concatenate([y_train, y_val])\n",
    "    # use best parameter to train the model\n",
    "    final_model = base_model.set_params(**best_params)\n",
    "    final_model.fit(X_trainval, y_trainval)\n",
    "    # predict on the test set and calculate the score\n",
    "    y_test_pred = final_model.predict(X_test_prep)\n",
    "    test_score = score_func(y_test, y_test_pred)\n",
    "\n",
    "    # For regression: use R² as the final evaluation metric (consistent with lecture and default sklearn scoring)\n",
    "    return final_model, best_params, test_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706733a7-d4ca-43bb-9dd0-d48bf78debfc",
   "metadata": {},
   "source": [
    "## Problem 2: time series forecasting with VAR\n",
    "\n",
    "You will practice multivariate time series forcasting using VAR - vector autoregression.\n",
    "\n",
    "The `stocks_prices.csv` is in the data folder. It contains the stock prices of amazon (AMZN), microsoft (MSFT), and apple (AAPL). Here is a description of each column in the dataset:\n",
    "- price ticker date: the date when the stock price was recorded - note that weekends and holidays are absent\n",
    "- Close AAPL: apple stock price at closing time in USD (i.e., at the end of the trading day)\n",
    "- Close AMZN: amazon stock price at closing time in USD (i.e., at the end of the trading day)\n",
    "- Close MSFT: microsoft stock price at closing time in USD (i.e., at the end of the trading day)\n",
    "- High AAPL: highest apple stock price during the trading day in USD\n",
    "- High AMZN: highest amazon stock price during the trading day in USD\n",
    "- High MSFT: highest microsoft stock price during the trading day in USD\n",
    "- Low AAPL: lowest apple stock price during the trading day in USD\n",
    "- Low AMZN: lowest amazon stock price during the trading day in USD\n",
    "- Low MSFT: lowest microsoft stock price during the trading day in USD\n",
    "- Open AAPL: apple stock price at opening time in USD (i.e., at the beginning of the trading day)\n",
    "- Open AMZN: amazon stock price at opening time in USD (i.e., at the beginning of the trading day)\n",
    "- Open MSFT: microsoft stock price opening time in USD (i.e., at the beginning of the trading day)\n",
    "- Volume AAPL: total traded volume (buys and sells) of apple on the trading day in USD\n",
    "- Volume AMZN: total traded volume (buys and sells) of amazon on the trading day in USD\n",
    "- Volume MSFT: total traded volume (buys and sells) of microsoft on the trading day in USD\n",
    "\n",
    "The goal of problem 2 is to predict the opening price of apple stocks one day ahead based on the time series observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00182271-de3b-4ed6-8e66-df74c4500aae",
   "metadata": {},
   "source": [
    "### Problem 2a - feature matrix (15 points)\n",
    "Perform the steps outlined in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49769466-6ab3-4c6b-a8e1-58e5dc9bd17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            close_aapl  close_amzn  close_msft  high_aapl  high_amzn  \\\n",
      "date                                                                   \n",
      "2020-01-02   72.538490   94.900497  153.042297  72.598869  94.900497   \n",
      "2020-01-03   71.833290   93.748497  151.136642  72.594055  94.309998   \n",
      "2020-01-06   72.405670   95.143997  151.527328  72.444313  95.184502   \n",
      "2020-01-07   72.065163   95.343002  150.145706  72.671356  95.694504   \n",
      "2020-01-08   73.224411   94.598503  152.537323  73.526303  95.550003   \n",
      "\n",
      "             high_msft   low_aapl   low_amzn    low_msft  open_aapl  \\\n",
      "date                                                                  \n",
      "2020-01-02  153.147108  71.292281  93.207497  150.860341  71.545867   \n",
      "2020-01-03  152.403898  71.608685  93.224998  150.603064  71.765667   \n",
      "2020-01-06  151.594033  70.703005  93.000000  149.126212  70.954181   \n",
      "2020-01-07  152.137101  71.845385  94.601997  149.897978  72.415353   \n",
      "2020-01-08  153.213833  71.768086  94.321999  150.498284  71.768086   \n",
      "\n",
      "            open_amzn   open_msft  volume_aapl  volume_amzn  volume_msft  \n",
      "date                                                                      \n",
      "2020-01-02  93.750000  151.289108    135480400     80580000     22622100  \n",
      "2020-01-03  93.224998  150.850807    146322800     75288000     21116200  \n",
      "2020-01-06  93.000000  149.669328    118387200     81236000     20813700  \n",
      "2020-01-07  95.224998  151.803622    108872000     80898000     21634100  \n",
      "2020-01-08  94.902000  151.432046    132079200     70160000     27746500  \n"
     ]
    }
   ],
   "source": [
    "# add your code here\n",
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# read file\n",
    "col_names = [\n",
    "    \"date\",\n",
    "    \"close_aapl\", \"close_amzn\", \"close_msft\",\n",
    "    \"high_aapl\", \"high_amzn\", \"high_msft\",\n",
    "    \"low_aapl\", \"low_amzn\", \"low_msft\",\n",
    "    \"open_aapl\", \"open_amzn\", \"open_msft\",\n",
    "    \"volume_aapl\", \"volume_amzn\", \"volume_msft\"]\n",
    "# we write the data from 4th row and rename the column\n",
    "df = pd.read_csv(\"data/stocks_prices.csv\", skiprows=3, names=col_names)\n",
    "# Convert date column to datetime, sort, and set as index\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values(\"date\").set_index(\"date\")\n",
    "# Print header of dataframe\n",
    "print(df.head())\n",
    "# Select the opening prices of AAPL, AMZN, and MSFT as multivariate inputs for the VAR model, since the goal is to forecast AAPL's next-day opening price.\n",
    "data = df[[\"open_aapl\", \"open_amzn\", \"open_msft\"]].copy()\n",
    "\n",
    "\n",
    "# write a function which takes the following input:\n",
    "# - the dataframe, \n",
    "# - the name of the target column, \n",
    "# - a variable `p` which describes how many autoregresive past features we use (p in AR(p) of the lecture notes)\n",
    "# the function should return a feature matrix X and target variable y after VAR is applied to the multivariate time series data\n",
    "# make sure that the points are ordered with respect to time such that \n",
    "# the oldest observations are at the top and the most recent observation are at the bottom of the dataframe.\n",
    "\n",
    "def build_VAR_features(dataframe, target_col, p):\n",
    "    \"\"\"\n",
    "    Build a feature matrix X and target y for a multivariate VAR(p) model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        Multivariate time series with all input variables (columns).\n",
    "    target_col : str\n",
    "        The column name of the variable we want to forecast (target).\n",
    "    p : int\n",
    "        Number of autoregressive lags (p in AR(p)).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : pandas.DataFrame\n",
    "        Feature matrix containing lagged values of all variables.\n",
    "        Ordered such that oldest observations are at the top\n",
    "        and most recent at the bottom.\n",
    "    y : pandas.Series\n",
    "        Target variable shifted by one step ahead of the input features.\n",
    "    \"\"\"\n",
    "    # Set X be a dataframe\n",
    "    X = pd.DataFrame()\n",
    "    # For each lag from 1 to p, shift all columns downward by 'lag' steps and rename columns as varname_lag1, varname_lag2, etc.\n",
    "    for lag in range(1, p + 1):\n",
    "        lagged = dataframe.shift(lag).copy()\n",
    "        lagged.columns = [f\"{col}_lag{lag}\" for col in dataframe.columns]\n",
    "        X = pd.concat([X, lagged], axis=1)\n",
    "    # Target is the original target column\n",
    "    y = dataframe[target_col].copy()\n",
    "    # Drop rows with missing values due to lagging\n",
    "    X = X.dropna()\n",
    "    y = y.loc[X.index]\n",
    "    # Return ordered with time increasing\n",
    "    return X, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b38e13-f080-4d49-a1d8-22d9efb7efe6",
   "metadata": {},
   "source": [
    "### Problem 2b - splitting (10 points)\n",
    "We will split the dataset in a couple of different ways to study information leakage. Perform the steps outlined in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d49515-ebb5-47dd-bb27-2019e766b77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------\n",
      " SPLIT 1: Random shuffled split\n",
      "--------------------------------\n",
      "\n",
      "Best Hyperparameter and Test R-square: best alpha=0.010, l1_ratio=1.00, test R2=0.9957\n",
      "Best Hyperparameter and Test R-square: best alpha=0.010, l1_ratio=1.00, test R2=0.9956\n",
      "Best Hyperparameter and Test R-square: best alpha=0.010, l1_ratio=1.00, test R2=0.9945\n",
      "Best Hyperparameter and Test R-square: best alpha=0.010, l1_ratio=0.78, test R2=0.9959\n",
      "Best Hyperparameter and Test R-square: best alpha=0.010, l1_ratio=0.78, test R2=0.9953\n",
      "\n",
      "Random split: mean test R² = 0.9954, std = 0.0005\n",
      "\n",
      "--------------------------------\n",
      " SPLIT 2: Time-series split\n",
      "--------------------------------\n",
      "\n",
      "Best Hyperparameter and Test R-square: best alpha=0.010, l1_ratio=0.78, test R2=0.8952\n",
      "Best Hyperparameter and Test R-square: best alpha=0.010, l1_ratio=1.00, test R2=0.9167\n",
      "Best Hyperparameter and Test R-square: best alpha=0.010, l1_ratio=1.00, test R2=0.9166\n",
      "Best Hyperparameter and Test R-square: best alpha=0.010, l1_ratio=1.00, test R2=0.9166\n",
      "Best Hyperparameter and Test R-square: best alpha=0.010, l1_ratio=1.00, test R2=0.9173\n",
      "\n",
      "TimeSeriesSplit: mean test R² = 0.9125, std = 0.0086\n"
     ]
    }
   ],
   "source": [
    "# add your code here\n",
    "\n",
    "# import library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "\n",
    "# Call build_VAR_features to get X and y\n",
    "X, y = build_VAR_features(data, target_col=\"open_aapl\", p=3)\n",
    "\n",
    "# split 1: create shuffled train/validation/test sets (60/20/20 ratio) and run `linear_ML_pipeline` on it. \n",
    "# print out the best hyperparameter values, and the test score\n",
    "# repeat this process with 5 random states\n",
    "# print out the best hyperparameter values, and the test score\n",
    "# print out the mean and stdev of the 5 test scores.\n",
    "print(\"\\n--------------------------------\")\n",
    "print(\" SPLIT 1: Random shuffled split\")\n",
    "print(\"--------------------------------\\n\")\n",
    "# Create a list to store test score\n",
    "test_scores_rand = []\n",
    "# Loop for 5 random states\n",
    "for seed in range(5):\n",
    "    # Split 60% training data, 20% validation data and 20% test data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=0.6, shuffle=True, random_state=seed)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size=0.5, shuffle=True, random_state=seed)\n",
    "\n",
    "    best_model, best_params, test_score = linear_ML_pipeline(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        is_classif=False, # not classification problem\n",
    "        continuous_ftrs=list(X.columns),  # all the features are continuous\n",
    "        ordinal_ftrs=[], ordinal_cats=[], categorical_ftrs=[]\n",
    "    )\n",
    "    test_scores_rand.append(test_score)\n",
    "    print(\"Best Hyperparameter and Test R-square: \"\n",
    "        f\"best alpha={float(best_params['alpha']):.3f}, \"\n",
    "      f\"l1_ratio={float(best_params['l1_ratio']):.2f}, \"\n",
    "      f\"test R2={test_score:.4f}\")\n",
    "\n",
    "# print mean and stdev\n",
    "print(f\"\\nRandom split: mean test R² = {np.mean(test_scores_rand):.4f}, \"\n",
    "      f\"std = {np.std(test_scores_rand):.4f}\")\n",
    "\n",
    "\n",
    "# split 2: place 20% of the most recent observations in the test set, \n",
    "# then apply sklearn's TimeSeriesSplit on the rest of the data with n_splits = 5\n",
    "# run `linear_ML_pipeline` on each split.\n",
    "# print out the best hyperparameter values, and the test score\n",
    "# print out the mean and stdev of the 5 test scores.\n",
    "print(\"\\n--------------------------------\")\n",
    "print(\" SPLIT 2: Time-series split\")\n",
    "print(\"--------------------------------\\n\")\n",
    "\n",
    "test_scores_time = []\n",
    "\n",
    "# let the last 20% data be test set\n",
    "n_test = int(0.2 * len(X))\n",
    "X_trainval, X_test = X.iloc[:-n_test], X.iloc[-n_test:]\n",
    "y_trainval, y_test = y.iloc[:-n_test], y.iloc[-n_test:]\n",
    "# Using TimeSeriesSplit to do the cross validation on the test of data set 'trainval'\n",
    "tscv = TimeSeriesSplit(n_splits=5) # n_splits = 5\n",
    "# Loop over each split\n",
    "for i, (train_idx, val_idx) in enumerate(tscv.split(X_trainval)):\n",
    "    # spilt the data\n",
    "    X_train, X_val = X_trainval.iloc[train_idx], X_trainval.iloc[val_idx]\n",
    "    y_train, y_val = y_trainval.iloc[train_idx], y_trainval.iloc[val_idx]\n",
    "\n",
    "    best_model, best_params, test_score = linear_ML_pipeline(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        is_classif=False,\n",
    "        continuous_ftrs=list(X.columns),\n",
    "        ordinal_ftrs=[], ordinal_cats=[], categorical_ftrs=[]\n",
    "    )\n",
    "    test_scores_time.append(test_score)\n",
    "    print(\"Best Hyperparameter and Test R-square: \"\n",
    "        f\"best alpha={float(best_params['alpha']):.3f}, \"\n",
    "      f\"l1_ratio={float(best_params['l1_ratio']):.2f}, \"\n",
    "      f\"test R2={test_score:.4f}\")\n",
    "\n",
    "print(f\"\\nTimeSeriesSplit: mean test R² = {np.mean(test_scores_time):.4f}, \"\n",
    "      f\"std = {np.std(test_scores_time):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7488498-2914-448d-a69e-43b166389eeb",
   "metadata": {},
   "source": [
    "Discuss in this cell what you observe and answer the questions below:\n",
    "\n",
    "- the mean and stdev of the test scores differ based on how you split the data. Explain in a few sentences why!\n",
    "\n",
    "**your answer here**\n",
    "\n",
    "The mean and standard deviation of the test R2 values differ because the two splitting methods capture different temporal structures in the data. In the random split, the training, validation, and test sets are randomly mixed, which causes data leakage，information from future time periods unintentionally enters the training set. This leakage allows the model to indirectly “see” future patterns, leading to artificially high R² scores (=0.995) and very low variance across random states. In contrast, the time-series split preserves the chronological order—training only on earlier data and testing on later observations—thus preventing data leakage and reflecting real forecasting conditions where future information is unavailable. As a result, the average R2 (=0.913) is lower and the variability is higher, but the evaluation is more realistic and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a51215-6434-4aa6-906a-57436b3000c9",
   "metadata": {},
   "source": [
    "- Forecasting models like this can be used for trading. I.e., if your model predicts that the opening price of the apple stock tomorrow will be higher than the closing price today, you'd put in a sell order at the end of the day. If your prediction is correct, you will make a profit in USD once your order executes at tomorrow's open. Similarly, if your model predicts that the opening price of apple stock tomorrow will be lower than the closing price today, you'd put in a buy order. If your prediction is correct, you'll buy at a low price once your order executes at tomorrow's open. This is how you'd act based on the model's prediction - buy low, sell high. Would you be willing to use your own money to deploy the models developed in split 1 and/or split 2? Why or why not?\n",
    "\n",
    "**your answer here**\n",
    "\n",
    "\n",
    "I would not use my own money to trade based on either model. The random-split(split 1) model is clearly overfitted because it benefits from data leakage and produces unrealistically high performance. The time-series(split 2) model performs more reasonably but still relies solely on past opening prices, ignoring market volatility, external news and any other critical factors in real trading. Although the time-series model is more trustworthy than the random-split one, both lack robustness and domain awareness, so they would not be suitable for real-world investment decisions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded0881",
   "metadata": {},
   "source": [
    "## Problem 3 - group structure\n",
    "\n",
    "We will work with the [hand postures dataset](https://archive.ics.uci.edu/ml/datasets/Motion+Capture+Hand+Postures) in problem 3. Please carefully read the dataset description and perform as much EDA as you can on this dataset. The EDA is not graded but it will help you to correctly answer 3a and 3b.\n",
    "\n",
    "This dataset has group structure: 14 users performing 5 different hand postures while wearing sensors attached to a left-handed glove. Two different ML questions can be asked using this dataset. We will explore how splitting differs for both questions in 2a and 2b.\n",
    "\n",
    "Later on, we'll teach you how to deal with missing data. For now, simply drop all columns with any missing values (**don't do this in real life**, but for now it's fine). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6443c3ab-843d-429f-8957-a1f8476601ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class  User         X0         Y0         Z0         X1         Y1  \\\n",
      "1      1     0  54.263880  71.466776 -64.807709  76.895635  42.462500   \n",
      "2      1     0  56.527558  72.266609 -61.935252  39.135978  82.538530   \n",
      "3      1     0  55.849928  72.469064 -62.562788  37.988804  82.631347   \n",
      "4      1     0  55.329647  71.707275 -63.688956  36.561863  81.868749   \n",
      "5      1     0  55.142401  71.435607 -64.177303  36.175818  81.556874   \n",
      "\n",
      "          Z1         X2         Y2         Z2  \n",
      "1 -72.780545  36.621229  81.680557 -52.919272  \n",
      "2 -49.596509  79.223743  43.254091 -69.982489  \n",
      "3 -50.606259  78.451526  43.567403 -70.658489  \n",
      "4 -52.752784  86.320630  68.214645 -72.228461  \n",
      "5 -53.475747  76.986143  42.426849 -72.574743  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHqCAYAAADVi/1VAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAeYJJREFUeJzs3XtclGX+//H3yFnCCTAYKTwVnkLNtEW0TU1FTaSysqJIt1JbT5GaZW6F7Sab5aEwtVxTVzTb3dJKi8RDlusZo9RczTIPJWKJ4wkB4f794Y/76wh4xHs4vJ6Px/14OPf9ue/rc82M4+Vnrrlum2EYhgAAAAAAAAAL1XB3AgAAAAAAAKh+KEoBAAAAAADAchSlAAAAAAAAYDmKUgAAAAAAALAcRSkAAAAAAABYjqIUAAAAAAAALEdRCgAAAAAAAJajKAUAAAAAAADLUZQCAAAAAACA5ShKocqbPXu2bDabufn6+srhcKhTp05KTk5WdnZ2iXOSkpJks9kuqZ2TJ08qKSlJX3755SWdV1pb9evXV2xs7CVd50Lmz5+vyZMnl3rMZrMpKSmpXNsrb8uXL1ebNm3k7+8vm82mRYsWlRr3888/y2az6Y033ij1+BtvvCGbzaaff/756iV7HhfzXBf3oXirUaOGgoODddddd2nt2rVXJa+pU6dq9uzZV+Xa5eXTTz9Vr169FBoaKm9vbwUFBalz586aN2+eCgoKzLjK8H4GAFQejCXPqC5jyS+//FI2m03/+c9/Sj0+ZMiQS35tAZSNohSqjVmzZmnt2rVKT0/X22+/rVtuuUWvvfaamjZtqmXLlrnEPvnkk5f8n/+TJ09q7NixlzyQuJy2Lsf5BhJr167Vk08+edVzuFyGYahPnz7y8vLSJ598orVr16pDhw7uTuuqGzp0qNauXauvv/5aycnJ+vbbb9WpUyd988035d5WRS5KGYahP/3pT4qLi1NRUZEmTpyoZcuWac6cOWrZsqUGDRqkqVOnujtNAEAVx1iSsSSA8ufp7gQAq0RGRqpNmzbm4/vuu0/PPPOMbr/9dvXu3Vs//PCDQkNDJUk33HCDbrjhhquaz8mTJ1WzZk1L2rqQtm3burX9C/n11191+PBh3XvvvercubO707FM3bp1zdemffv2uummm9S5c2dNnTpVM2bMcHN2F6f4fX4lXn/9dc2ePVtjx47VSy+95HKsV69eGjVqlHbt2nVFbQAAcCGMJcvGWPLqMQxDp06dkp+fn7tTAa4KZkqhWqtbt64mTJigY8eO6Z133jH3lzYNesWKFerYsaOCg4Pl5+enunXr6r777tPJkyf1888/67rrrpMkjR071pze3a9fP5frbd68Wffff78CAwN14403ltlWsYULF6pFixby9fVVw4YN9dZbb7kcL55Ofu5P0YqnHRd/09axY0ctWbJEe/bscZl+Xqy0Kddbt27V3XffrcDAQPn6+uqWW27RnDlzSm3n/fff15gxYxQWFqZatWqpS5cu2rFjR9lP/FlWr16tzp07KyAgQDVr1lS7du20ZMkS83hSUpI50Hruuedks9lUv379i7r2xUpPT9fdd9+tG264Qb6+vrrppps0cOBA/fbbby5xxa/Vtm3b9PDDD8tutys0NFSPP/64nE6nS+zRo0fVv39/BQcH65prrlH37t21c+fOK8qzeMC3Z88ec997772nli1bytfXV0FBQbr33nu1fft2l/N++uknPfTQQwoLC5OPj49CQ0PVuXNnZWZmSjozxX/btm1atWqV+d4ofo4v9j0mnXmfRUZG6quvvlK7du1Us2ZNPf744+bzMXLkSDVo0EDe3t66/vrrlZiYqBMnTpy3zwUFBXrttdfUpEkTvfjii6XGOBwO3X777WVe49ChQxo0aJCaNWuma665RiEhIbrzzjv19ddfl4idNm2aWrZsqWuuuUYBAQFq0qSJXnjhBfP4yZMnzX4UP+dt2rTR+++/73KdTZs2KS4uTkFBQfL19VWrVq30r3/9yyXmYq8FAKi4GEueUd3Hkv/+978VFRUlu92umjVrqmHDhuYYqNjFjoVsNpuGDBmi6dOnq2nTpvLx8TGftwuNU4DKiJlSqPbuuusueXh46Kuvvioz5ueff1bPnj31xz/+Ue+9956uvfZa/fLLL0pLS1N+fr7q1KmjtLQ0de/eXU888YQ5fbl4cFGsd+/eeuihh/TUU09d8D/jmZmZSkxMVFJSkhwOh+bNm6enn35a+fn5Gjly5CX1cerUqRowYIB+/PFHLVy48ILxO3bsULt27RQSEqK33npLwcHBSk1NVb9+/XTw4EGNGjXKJf6FF15Q+/bt9Y9//ENHjx7Vc889p169emn79u3y8PAos51Vq1apa9euatGihWbOnCkfHx9NnTpVvXr10vvvv68HH3xQTz75pFq2bKnevXtr6NChio+Pl4+PzwX7UFRUpNOnT5e6/1w//vijoqOj9eSTT8put+vnn3/WxIkTdfvtt2vLli3y8vJyib/vvvv04IMP6oknntCWLVs0evRoSWcKRNKZb7TuuecerVmzRi+99JJuu+02/fe//1WPHj0umPf5FM8GKn5fJScn64UXXtDDDz+s5ORk/f7770pKSlJ0dLQ2btyoiIgISWfe44WFhRo/frzq1q2r3377TWvWrNGRI0cknRmw3n///bLb7ebP4C7mOS7NgQMH9Oijj2rUqFEaN26catSooZMnT6pDhw7av3+/XnjhBbVo0ULbtm3TSy+9pC1btmjZsmVlDqY3bdqkw4cPq3///pe9fsPhw4clSS+//LIcDoeOHz+uhQsXqmPHjlq+fLk6duwoSVqwYIEGDRqkoUOH6o033lCNGjW0a9cuff/99+a1hg8frrlz5+pvf/ubWrVqpRMnTmjr1q36/fffzZiVK1eqe/fuioqK0vTp02W327VgwQI9+OCDOnnypPkfjIu5FgCg4mMsWVJVGEterLVr1+rBBx/Ugw8+qKSkJPn6+mrPnj1asWKFGXOpY6FFixbp66+/1ksvvSSHw6GQkJCLGqcAlZIBVHGzZs0yJBkbN24sMyY0NNRo2rSp+fjll182zv7r8Z///MeQZGRmZpZ5jUOHDhmSjJdffrnEseLrvfTSS2UeO1u9evUMm81Wor2uXbsatWrVMk6cOOHSt927d7vErVy50pBkrFy50tzXs2dPo169eqXmfm7eDz30kOHj42Ps3bvXJa5Hjx5GzZo1jSNHjri0c9ddd7nE/etf/zIkGWvXri21vWJt27Y1QkJCjGPHjpn7Tp8+bURGRho33HCDUVRUZBiGYezevduQZLz++uvnvd7ZsRfazn3OihUVFRkFBQXGnj17DEnGxx9/bB4rfq3Gjx/vcs6gQYMMX19fM9/PP//ckGS8+eabLnGvvvpqme+R0vrw2muvGQUFBcapU6eMjIwM47bbbjMkGUuWLDFycnIMPz+/Es/93r17DR8fHyM+Pt4wDMP47bffDEnG5MmTz9vmzTffbHTo0KHE/kt5j3Xo0MGQZCxfvtwlNjk52ahRo0aJv4PFf68+++yzMvNasGCBIcmYPn36efM/24We49OnTxsFBQVG586djXvvvdfcP2TIEOPaa68977UjIyONe+6557wxTZo0MVq1amUUFBS47I+NjTXq1KljFBYWXvS1AADux1jyjOoylizO6d///nepxwcPHuzyfL/xxhuGJLNPpbmUsZAkw263G4cPH3aJvZhxClAZ8fM9QGdmtpzPLbfcIm9vbw0YMEBz5szRTz/9dFnt3HfffRcde/PNN6tly5Yu++Lj43X06FFt3rz5stq/WCtWrFDnzp0VHh7usr9fv346efJkicU04+LiXB63aNFCkuvPzM514sQJrV+/Xvfff7+uueYac7+Hh4cSEhK0f//+i562XZqnn35aGzduLLE9/fTTJWKzs7P11FNPKTw8XJ6envLy8lK9evUkqcRP4aTS+3vq1Cnz7jsrV66UJD3yyCMucfHx8ZfUh+eee05eXl7y9fVV69attXfvXr3zzjvmXfhyc3PNWTfFwsPDdeedd2r58uWSpKCgIN144416/fXXNXHiRH3zzTelzhYrL4GBgbrzzjtd9i1evFiRkZG65ZZbdPr0aXPr1q1biZ8AXi3Tp0/XrbfeKl9fX/M1Xr58ucvr+4c//EFHjhzRww8/rI8//rjEzzeLYz7//HM9//zz+vLLL5Wbm+tyfNeuXfrf//5nvvZn9/euu+7SgQMHzPf1ha4FAKg8GEu6qgpjyYt12223SZL69Omjf/3rX/rll19KxFzqWOjOO+9UYGCgy76LGacAlRFFKVR7J06c0O+//66wsLAyY2688UYtW7ZMISEhGjx4sG688UbdeOONevPNNy+prTp16lx0rMPhKHPf1f55z++//15qrsXP0bntBwcHuzwunhJ9vv9k5+TkyDCMS2rnUtxwww1q06ZNie3chUCLiooUExOjjz76SKNGjdLy5cu1YcMGrVu3rsw+XKi/v//+uzw9PUvElfaank9xYS0jI0M//vijDhw4oAEDBphtSKW/p8LCwszjNptNy5cvV7du3TR+/Hjdeuutuu666zRs2DAdO3bskvK5GKXlc/DgQX333Xfy8vJy2QICAmQYxnkHVXXr1pUk7d69+7Jzmjhxov785z8rKipKH374odatW6eNGzeqe/fuLq9vQkKC3nvvPe3Zs0f33XefQkJCFBUVpfT0dDPmrbfe0nPPPadFixapU6dOCgoK0j333KMffvjB7KskjRw5skR/Bw0aJElmfy90LQBA5cBYsqTKPJb09Dyzwk1hYWGpx0+fPm3GSNIdd9yhRYsW6fTp03rsscd0ww03KDIy0mWNyEsdC5XWp4sZpwCVEWtKodpbsmSJCgsLzXVlyvLHP/5Rf/zjH1VYWKhNmzYpJSVFiYmJCg0N1UMPPXRRbV3KmjhZWVll7iv+h9vX11eSlJeX5xJ3pd+cBAcH68CBAyX2//rrr5Kk2rVrX9H1pTMzamrUqHHV27mQrVu36ttvv9Xs2bPVt29fc/+V3M0tODhYp0+f1u+//+4yyCrtNT2f4sJaWW1IKvP5O/u5q1evnmbOnClJ2rlzp/71r38pKSlJ+fn5mj59+nlzuNT3WGnv8dq1a8vPz89cc6u042Vp06aNgoKC9PHHHys5Ofmy1pVKTU1Vx44dNW3aNJf9pRXl/vSnP+lPf/qTTpw4oa+++kovv/yyYmNjtXPnTtWrV0/+/v4aO3asxo4dq4MHD5oznXr16qX//e9/Zl9Gjx6t3r17l5pP48aNJemC1wIAVA6MJUuqzGPJ4jsoljbjqXh/cUyxu+++W3fffbfy8vK0bt06JScnKz4+XvXr11d0dPQlj4XKep0vNE4BKiNmSqFa27t3r0aOHCm73a6BAwde1DkeHh6KiorS22+/LUnm9OeL+UbnUmzbtk3ffvuty7758+crICBAt956qySZdw757rvvXOI++eSTEtfz8fG56Nw6d+6sFStWmP+gF/vnP/+pmjVrlsttf/39/RUVFaWPPvrIJa+ioiKlpqbqhhtuUKNGja64nQsp/kf/3AUvz76DzqXq1KmTJGnevHku++fPn3/Z1zxXdHS0/Pz8lJqa6rJ///795pT50jRq1Eh/+ctf1Lx5c5ep+2W9Py7lPVaW2NhY/fjjjwoODi519tr57oDj5eWl5557Tv/73//017/+tdSY7Oxs/fe//y3zGjabrcTr+91335X46cDZ/P391aNHD40ZM0b5+fnatm1biZjQ0FD169dPDz/8sHbs2KGTJ0+qcePGioiI0LfffltqX9u0aaOAgICLuhYAoOJjLFm6yjyWjIiIUL169fTvf/+7xM8yDx06pJUrV6pLly6lnuvj46MOHTrotddekyR98803kq5sLFSaixmnAJUFM6VQbWzdutX8/XZ2dra+/vprzZo1Sx4eHlq4cGGJu5ucbfr06VqxYoV69uypunXr6tSpU+Y3HcX/KAUEBKhevXr6+OOP1blzZwUFBal27dqXfcvZsLAwxcXFKSkpSXXq1FFqaqrS09P12muvqWbNmpLO/Ia9cePGGjlypE6fPq3AwEAtXLhQq1evLnG95s2b66OPPtK0adPUunVr1ahRo8xZOC+//LIWL16sTp066aWXXlJQUJDmzZunJUuWaPz48bLb7ZfVp3MlJyera9eu6tSpk0aOHClvb29NnTpVW7du1fvvv3/Zd1u7FE2aNNGNN96o559/XoZhKCgoSJ9++ukVTYWOiYnRHXfcoVGjRunEiRNq06aN/vvf/2ru3Lnllve1116rF198US+88IIee+wxPfzww/r99981duxY+fr66uWXX5Z0ZpA5ZMgQPfDAA4qIiJC3t7dWrFih7777Ts8//7x5vebNm2vBggX64IMP1LBhQ/n6+qp58+aX9B4rS2Jioj788EPdcccdeuaZZ9SiRQsVFRVp7969Wrp0qUaMGKGoqKgyz3/22We1fft2vfzyy9qwYYPi4+MVHh4up9Opr776Su+++67Gjh2r9u3bl3p+bGys/vrXv+rll19Whw4dtGPHDr3yyitq0KCByx0a+/fvLz8/P7Vv31516tRRVlaWkpOTZbfbzfUioqKiFBsbqxYtWigwMFDbt2/X3LlzFR0dbf69fOedd9SjRw9169ZN/fr10/XXX6/Dhw9r+/bt2rx5s/79739f9LUAABUHY8nqM5Z844031KdPH3Xu3Fn9+/eXw+HQDz/8oL///e/y9vbWiy++aMa+9NJL2r9/vzp37qwbbrhBR44c0ZtvvikvLy916NBB0pWPhaSLG6cAlZL71lgHrFF8V5Hizdvb2wgJCTE6dOhgjBs3zsjOzi5xzrl3MVm7dq1x7733GvXq1TN8fHyM4OBgo0OHDsYnn3zict6yZcuMVq1aGT4+PoYko2/fvi7XO3To0AXbMowzd0zp2bOn8Z///Me4+eabDW9vb6N+/frGxIkTS5y/c+dOIyYmxqhVq5Zx3XXXGUOHDjWWLFlS4o4phw8fNu6//37j2muvNWw2m0ubKuVOL1u2bDF69epl2O12w9vb22jZsqUxa9Ysl5iy7k5SfIeTc+NL8/XXXxt33nmn4e/vb/j5+Rlt27Y1Pv3001Kvdyl33ysr9vXXXy9xl5nvv//e6Nq1qxEQEGAEBgYaDzzwgLF3794Sz0tZr2Npd645cuSI8fjjjxvXXnutUbNmTaNr167G//73v0u6+97F9Pcf//iH0aJFC8Pb29uw2+3G3XffbWzbts08fvDgQaNfv35GkyZNDH9/f+Oaa64xWrRoYUyaNMk4ffq0Gffzzz8bMTExRkBAgCHJ5e46F/se69Chg3HzzTeXmufx48eNv/zlL0bjxo3NXJs3b24888wzRlZW1gX7aRiG8fHHHxs9e/Y0rrvuOsPT09MIDAw0OnXqZEyfPt3Iy8sz4859jvPy8oyRI0ca119/veHr62vceuutxqJFi4y+ffu69HPOnDlGp06djNDQUMPb29sICwsz+vTpY3z33XdmzPPPP2+0adPGCAwMNHx8fIyGDRsazzzzjPHbb7+55Prtt98affr0MUJCQgwvLy/D4XAYd955p8tdBC/2WgAA92IseUZ1GUsWW7ZsmRETE2Nce+21hqenp1GnTh3j0UcfNX744QeXuMWLFxs9evQwrr/+evO9cddddxlff/21S9zFjoUkGYMHDy6Rz8WMU4DKyGYYF7hVBAAAAAAAAFDOWFMKAAAAAAAAlqMoBQAAAAAAAMtRlAIAAAAAAIDlKEoBAAAAAADAchSlAAAAAAAAYDmKUgAAAJXMV199pV69eiksLEw2m02LFi1yOW4YhpKSkhQWFiY/Pz917NhR27Ztc4nJy8vT0KFDVbt2bfn7+ysuLk779+93icnJyVFCQoLsdrvsdrsSEhJ05MgRl5i9e/eqV69e8vf3V+3atTVs2DDl5+dfjW4DAIAqxtPdCVQlRUVF+vXXXxUQECCbzebudAAAQDkwDEPHjh1TWFiYatSoGN/nnThxQi1bttSf/vQn3XfffSWOjx8/XhMnTtTs2bPVqFEj/e1vf1PXrl21Y8cOBQQESJISExP16aefasGCBQoODtaIESMUGxurjIwMeXh4SJLi4+O1f/9+paWlSZIGDBighIQEffrpp5KkwsJC9ezZU9ddd51Wr16t33//XX379pVhGEpJSbmovjB+AgCg6rno8ZOBcrNv3z5DEhsbGxsbG1sV3Pbt2+fuoUapJBkLFy40HxcVFRkOh8P4+9//bu47deqUYbfbjenTpxuGYRhHjhwxvLy8jAULFpgxv/zyi1GjRg0jLS3NMAzD+P777w1Jxrp168yYtWvXGpKM//3vf4ZhGMZnn31m1KhRw/jll1/MmPfff9/w8fExnE7nReXP+ImNjY2Nja3qbhcaPzFTqhwVf/O4b98+1apVy83ZAACA8nD06FGFh4eb/85XdLt371ZWVpZiYmLMfT4+PurQoYPWrFmjgQMHKiMjQwUFBS4xYWFhioyM1Jo1a9StWzetXbtWdrtdUVFRZkzbtm1lt9u1Zs0aNW7cWGvXrlVkZKTCwsLMmG7duikvL08ZGRnq1KlTifzy8vKUl5dnPj5TV2P8BABAVXKx4yeKUuWoeMp5rVq1GFQBAFDFVJaflmVlZUmSQkNDXfaHhoZqz549Zoy3t7cCAwNLxBSfn5WVpZCQkBLXDwkJcYk5t53AwEB5e3ubMedKTk7W2LFjS+xn/AQAQNVzofFTxVgYAQAAAOXq3EGgYRgXHBieG1Na/OXEnG306NFyOp3mtm/fvvPmBAAAqi6KUgAAAFWIw+GQpBIzlbKzs81ZTQ6HQ/n5+crJyTlvzMGDB0tc/9ChQy4x57aTk5OjgoKCEjOoivn4+JizopgdBQBA9UZRCgAAoApp0KCBHA6H0tPTzX35+flatWqV2rVrJ0lq3bq1vLy8XGIOHDigrVu3mjHR0dFyOp3asGGDGbN+/Xo5nU6XmK1bt+rAgQNmzNKlS+Xj46PWrVtf1X4CAIDKjzWlAAAAKpnjx49r165d5uPdu3crMzNTQUFBqlu3rhITEzVu3DhFREQoIiJC48aNU82aNRUfHy9JstvteuKJJzRixAgFBwcrKChII0eOVPPmzdWlSxdJUtOmTdW9e3f1799f77zzjiRpwIABio2NVePGjSVJMTExatasmRISEvT666/r8OHDGjlypPr3788MKAAAcEEUpQAAACqZTZs2udzZbvjw4ZKkvn37avbs2Ro1apRyc3M1aNAg5eTkKCoqSkuXLnW5A86kSZPk6empPn36KDc3V507d9bs2bPl4eFhxsybN0/Dhg0z79IXFxenKVOmmMc9PDy0ZMkSDRo0SO3bt5efn5/i4+P1xhtvXO2nAAAAVAE2o/g+vLhiR48eld1ul9Pp5NtBAACqCP59v7p4fgEAqHou9t931pQCAAAAAACA5ShKAQAAAAAAwHIUpQAAAAAAAGA5ilIAAAAAAACwHEUpAAAAAAAAWI6iFAAAAAAAACxHUQoAAAAAAACWoygFAAAAAAAAy1GUAgAAAAAAgOUoSgEAAAAAAMByFKUAAAAAAABgOU93JwAApZmautrdKVR6gx693d0pAAAA4Ap89tpnlrZ313N3WdoewEwpAAAAAAAAWI6ZUm4y4vN/ujuFSm1Cj8fcnQIAAAAAALgCbp0p9dVXX6lXr14KCwuTzWbTokWLSsRs375dcXFxstvtCggIUNu2bbV3717zeF5enoYOHaratWvL399fcXFx2r9/v8s1cnJylJCQILvdLrvdroSEBB05csQlZu/everVq5f8/f1Vu3ZtDRs2TPn5+Vej2wAAAAAAANWeW4tSJ06cUMuWLTVlypRSj//444+6/fbb1aRJE3355Zf69ttv9eKLL8rX19eMSUxM1MKFC7VgwQKtXr1ax48fV2xsrAoLC82Y+Ph4ZWZmKi0tTWlpacrMzFRCQoJ5vLCwUD179tSJEye0evVqLViwQB9++KFGjBhx9ToPAAAAAABQjbn153s9evRQjx49yjw+ZswY3XXXXRo/fry5r2HDhuafnU6nZs6cqblz56pLly6SpNTUVIWHh2vZsmXq1q2btm/frrS0NK1bt05RUVGSpBkzZig6Olo7duxQ48aNtXTpUn3//ffat2+fwsLCJEkTJkxQv3799Oqrr6pWrVpXo/sAAAAAAADVVoVd6LyoqEhLlixRo0aN1K1bN4WEhCgqKsrlJ34ZGRkqKChQTEyMuS8sLEyRkZFas2aNJGnt2rWy2+1mQUqS2rZtK7vd7hITGRlpFqQkqVu3bsrLy1NGRsZV7ikAAAAAAED1U2GLUtnZ2Tp+/Lj+/ve/q3v37lq6dKnuvfde9e7dW6tWrZIkZWVlydvbW4GBgS7nhoaGKisry4wJCQkpcf2QkBCXmNDQUJfjgYGB8vb2NmNKk5eXp6NHj7psAAAAAAAAuLAKe/e9oqIiSdLdd9+tZ555RpJ0yy23aM2aNZo+fbo6dOhQ5rmGYchms5mPz/7zlcScKzk5WWPHjr1wZwAAAAAAAOCiws6Uql27tjw9PdWsWTOX/U2bNjXvvudwOJSfn6+cnByXmOzsbHPmk8Ph0MGDB0tc/9ChQy4x586IysnJUUFBQYkZVGcbPXq0nE6nue3bt+/SOwoAAAAAAFANVdiilLe3t2677Tbt2LHDZf/OnTtVr149SVLr1q3l5eWl9PR08/iBAwe0detWtWvXTpIUHR0tp9OpDRs2mDHr16+X0+l0idm6dasOHDhgxixdulQ+Pj5q3bp1mTn6+PioVq1aLhsAAAAAAAAuzK0/3zt+/Lh27dplPt69e7cyMzMVFBSkunXr6tlnn9WDDz6oO+64Q506dVJaWpo+/fRTffnll5Iku92uJ554QiNGjFBwcLCCgoI0cuRINW/e3LwbX9OmTdW9e3f1799f77zzjiRpwIABio2NVePGjSVJMTExatasmRISEvT666/r8OHDGjlypPr370+hCQAAAAAA4Cpw60ypTZs2qVWrVmrVqpUkafjw4WrVqpVeeuklSdK9996r6dOna/z48WrevLn+8Y9/6MMPP9Ttt99uXmPSpEm655571KdPH7Vv3141a9bUp59+Kg8PDzNm3rx5at68uWJiYhQTE6MWLVpo7ty55nEPDw8tWbJEvr6+at++vfr06aN77rlHb7zxhkXPBAAAAAAAQPXi1plSHTt2lGEY5415/PHH9fjjj5d53NfXVykpKUpJSSkzJigoSKmpqedtp27dulq8ePH5EwYAAAAAAEC5qLB33wMAAOc3d/0Ad6dQqSVEvevuFAAAAKq1CrvQOQAAAAAAAKouilIAAAAAAACwHEUpAAAAAAAAWI6iFAAAAAAAACxHUQoAAAAAAACWoygFAAAAAAAAy3m6OwEAAADgbNM2fmV5m3++7Q7L2wQAoLpjphQAAAAAAAAsR1EKAAAAAAAAlqMoBQAAAAAAAMtRlAIAAAAAAIDlKEoBAAAAAADAchSlAAAAAAAAYDmKUgAAAAAAALAcRSkAAAAAAABYjqIUAAAAAAAALEdRCgAAAAAAAJbzdHcCQEWQPW2Uu1Oo9EL+PN7dKQAAAAAAKhGKUgAAAAAAAFdo2savLG3vz7fdYWl7VwM/3wMAAAAAAIDlKEoBAAAAAADAchSlAAAAAAAAYDnWlAIAXJRd01a5O4VK76Y/d3B3CgAAAECFwUwpAAAAAAAAWI6iFAAAAAAAACxHUQoAAAAAAACWoygFAAAAAAAAy1GUAgAAAAAAgOUoSgEAAAAAAMByFKUAAAAAAABgOYpSAAAAAAAAsBxFKQAAAAAAAFiOohQAAAAAAAAsR1EKAAAAAAAAlqMoBQAAAAAAAMtRlAIAAAAAAIDl3FqU+uqrr9SrVy+FhYXJZrNp0aJFZcYOHDhQNptNkydPdtmfl5enoUOHqnbt2vL391dcXJz279/vEpOTk6OEhATZ7XbZ7XYlJCToyJEjLjF79+5Vr1695O/vr9q1a2vYsGHKz88vp54CAAAAAADgbG4tSp04cUItW7bUlClTzhu3aNEirV+/XmFhYSWOJSYmauHChVqwYIFWr16t48ePKzY2VoWFhWZMfHy8MjMzlZaWprS0NGVmZiohIcE8XlhYqJ49e+rEiRNavXq1FixYoA8//FAjRowov84CAAAAAADA5OnOxnv06KEePXqcN+aXX37RkCFD9MUXX6hnz54ux5xOp2bOnKm5c+eqS5cukqTU1FSFh4dr2bJl6tatm7Zv3660tDStW7dOUVFRkqQZM2YoOjpaO3bsUOPGjbV06VJ9//332rdvn1n4mjBhgvr166dXX31VtWrVugq9BwAAAAAAqL4q9JpSRUVFSkhI0LPPPqubb765xPGMjAwVFBQoJibG3BcWFqbIyEitWbNGkrR27VrZ7XazICVJbdu2ld1ud4mJjIx0mYnVrVs35eXlKSMjo8z88vLydPToUZcNAAAAAAAAF1ahi1KvvfaaPD09NWzYsFKPZ2VlydvbW4GBgS77Q0NDlZWVZcaEhISUODckJMQlJjQ01OV4YGCgvL29zZjSJCcnm+tU2e12hYeHX1L/AAAAAAAAqqsKW5TKyMjQm2++qdmzZ8tms13SuYZhuJxT2vmXE3Ou0aNHy+l0mtu+ffsuKU8AAAAAAIDqqsIWpb7++mtlZ2erbt268vT0lKenp/bs2aMRI0aofv36kiSHw6H8/Hzl5OS4nJudnW3OfHI4HDp48GCJ6x86dMgl5twZUTk5OSooKCgxg+psPj4+qlWrlssGAAAAAACAC6uwRamEhAR99913yszMNLewsDA9++yz+uKLLyRJrVu3lpeXl9LT083zDhw4oK1bt6pdu3aSpOjoaDmdTm3YsMGMWb9+vZxOp0vM1q1bdeDAATNm6dKl8vHxUevWra3oLgAAAAAAQLXi1qLU8ePHzYKTJO3evVuZmZnau3evgoODFRkZ6bJ5eXnJ4XCocePGkiS73a4nnnhCI0aM0PLly/XNN9/o0UcfVfPmzc278TVt2lTdu3dX//79tW7dOq1bt079+/dXbGyseZ2YmBg1a9ZMCQkJ+uabb7R8+XKNHDlS/fv3Z/YTAACodE6fPq2//OUvatCggfz8/NSwYUO98sorKioqMmMMw1BSUpLCwsLk5+enjh07atu2bS7XycvL09ChQ1W7dm35+/srLi5O+/fvd4nJyclRQkKCucZmQkKCjhw5YkU3AQBAJefWotSmTZvUqlUrtWrVSpI0fPhwtWrVSi+99NJFX2PSpEm655571KdPH7Vv3141a9bUp59+Kg8PDzNm3rx5at68uWJiYhQTE6MWLVpo7ty55nEPDw8tWbJEvr6+at++vfr06aN77rlHb7zxRvl1FgAAwCKvvfaapk+frilTpmj79u0aP368Xn/9daWkpJgx48eP18SJEzVlyhRt3LhRDodDXbt21bFjx8yYxMRELVy4UAsWLNDq1at1/PhxxcbGqrCw0IyJj49XZmam0tLSlJaWpszMTCUkJFjaXwAAUDl5urPxjh07yjCMi47/+eefS+zz9fVVSkqKyyDrXEFBQUpNTT3vtevWravFixdfdC4AAAAV1dq1a3X33XerZ8+ekqT69evr/fff16ZNmySdmSU1efJkjRkzRr1795YkzZkzR6GhoZo/f74GDhwop9OpmTNnau7cueYM9NTUVIWHh2vZsmXq1q2btm/frrS0NK1bt05RUVGSpBkzZig6Olo7duwwZ6UDAKqGzAMbLW3vljq3WdoerFdh15QCAADA5bn99tu1fPly7dy5U5L07bffavXq1brrrrsknVkyISsrSzExMeY5Pj4+6tChg9asWSPpzJ2QCwoKXGLCwsIUGRlpxqxdu1Z2u90sSElS27ZtZbfbzZhz5eXl6ejRoy4bAAContw6UwoAAADl77nnnpPT6VSTJk3k4eGhwsJCvfrqq3r44Yclybzr8Ll3GQ4NDdWePXvMGG9vbwUGBpaIKT4/KytLISEhJdoPCQkpcWfjYsnJyRo7duyVdRAAAFQJzJQCAACoYj744AOlpqZq/vz52rx5s+bMmaM33nhDc+bMcYmz2Wwujw3DKLHvXOfGlBZ/vuuMHj1aTqfT3Pbt23ex3QIAAFUMM6UAAACqmGeffVbPP/+8HnroIUlS8+bNtWfPHiUnJ6tv375yOBySzsx0qlOnjnledna2OXvK4XAoPz9fOTk5LrOlsrOz1a5dOzPm4MGDJdo/dOhQiVlYxXx8fOTj41M+HQUAAJUaM6UAAACqmJMnT6pGDddhnoeHh4qKiiRJDRo0kMPhUHp6unk8Pz9fq1atMgtOrVu3lpeXl0vMgQMHtHXrVjMmOjpaTqdTGzZsMGPWr18vp9NpxgAAAJSFmVIAAABVTK9evfTqq6+qbt26uvnmm/XNN99o4sSJevzxxyWd+cldYmKixo0bp4iICEVERGjcuHGqWbOm4uPjJUl2u11PPPGERowYoeDgYAUFBWnkyJFq3ry5eTe+pk2bqnv37urfv7/eeecdSdKAAQMUGxvLnfcAAMAFUZQCAACoYlJSUvTiiy9q0KBBys7OVlhYmAYOHKiXXnrJjBk1apRyc3M1aNAg5eTkKCoqSkuXLlVAQIAZM2nSJHl6eqpPnz7Kzc1V586dNXv2bHl4eJgx8+bN07Bhw8y79MXFxWnKlCnWdRYAAFRaFKUAAACqmICAAE2ePFmTJ08uM8ZmsykpKUlJSUllxvj6+iolJUUpKSllxgQFBSk1NfUKsgUAANUVa0oBAAAAAADAchSlAAAAAAAAYDmKUgAAAAAAALAcRSkAAAAAAABYjqIUAAAAAAAALEdRCgAAAAAAAJajKAUAAAAAAADLUZQCAAAAAACA5ShKAQAAAAAAwHIUpQAAAAAAAGA5ilIAAAAAAACwHEUpAAAAAAAAWI6iFAAAAAAAACxHUQoAAAAAAACW83R3AgAAAAAAAChfubnLLW3Pz6/zJZ/DTCkAAAAAAABYjqIUAAAAAAAALEdRCgAAAAAAAJajKAUAAAAAAADLUZQCAAAAAACA5ShKAQAAAAAAwHIUpQAAAAAAAGA5ilIAAAAAAACwHEUpAAAAAAAAWI6iFAAAAAAAACxHUQoAAAAAAACWoygFAAAAAAAAy1GUAgAAAAAAgOXcWpT66quv1KtXL4WFhclms2nRokXmsYKCAj333HNq3ry5/P39FRYWpscee0y//vqryzXy8vI0dOhQ1a5dW/7+/oqLi9P+/ftdYnJycpSQkCC73S673a6EhAQdOXLEJWbv3r3q1auX/P39Vbt2bQ0bNkz5+flXq+sAAAAAAADVmluLUidOnFDLli01ZcqUEsdOnjypzZs368UXX9TmzZv10UcfaefOnYqLi3OJS0xM1MKFC7VgwQKtXr1ax48fV2xsrAoLC82Y+Ph4ZWZmKi0tTWlpacrMzFRCQoJ5vLCwUD179tSJEye0evVqLViwQB9++KFGjBhx9ToPAAAAAABQjXm6s/EePXqoR48epR6z2+1KT0932ZeSkqI//OEP2rt3r+rWrSun06mZM2dq7ty56tKliyQpNTVV4eHhWrZsmbp166bt27crLS1N69atU1RUlCRpxowZio6O1o4dO9S4cWMtXbpU33//vfbt26ewsDBJ0oQJE9SvXz+9+uqrqlWr1lV8FgAAAAAAAKqfSrWmlNPplM1m07XXXitJysjIUEFBgWJiYsyYsLAwRUZGas2aNZKktWvXym63mwUpSWrbtq3sdrtLTGRkpFmQkqRu3bopLy9PGRkZFvQMAAAAAACgenHrTKlLcerUKT3//POKj483Zy5lZWXJ29tbgYGBLrGhoaHKysoyY0JCQkpcLyQkxCUmNDTU5XhgYKC8vb3NmNLk5eUpLy/PfHz06NHL6xwAAAAAAEA1UylmShUUFOihhx5SUVGRpk6desF4wzBks9nMx2f/+UpizpWcnGwunm632xUeHn7B3AAAAAAAAFAJilIFBQXq06ePdu/erfT0dJf1nRwOh/Lz85WTk+NyTnZ2tjnzyeFw6ODBgyWue+jQIZeYc2dE5eTkqKCgoMQMqrONHj1aTqfT3Pbt23fZ/QQAAAAAAKhOKnRRqrgg9cMPP2jZsmUKDg52Od66dWt5eXm5LIh+4MABbd26Ve3atZMkRUdHy+l0asOGDWbM+vXr5XQ6XWK2bt2qAwcOmDFLly6Vj4+PWrduXWZ+Pj4+qlWrlssGAAAAAACAC3PrmlLHjx/Xrl27zMe7d+9WZmamgoKCFBYWpvvvv1+bN2/W4sWLVVhYaM5mCgoKkre3t+x2u5544gmNGDFCwcHBCgoK0siRI9W8eXPzbnxNmzZV9+7d1b9/f73zzjuSpAEDBig2NlaNGzeWJMXExKhZs2ZKSEjQ66+/rsOHD2vkyJHq378/hSYAAAAAAICrwK1FqU2bNqlTp07m4+HDh0uS+vbtq6SkJH3yySeSpFtuucXlvJUrV6pjx46SpEmTJsnT01N9+vRRbm6uOnfurNmzZ8vDw8OMnzdvnoYNG2bepS8uLk5Tpkwxj3t4eGjJkiUaNGiQ2rdvLz8/P8XHx+uNN964Gt0GAAAAAACo9txalOrYsaMMwyjz+PmOFfP19VVKSopSUlLKjAkKClJqaup5r1O3bl0tXrz4gu0BAAAAAADgylXoNaUAAAAAAABQNVGUAgAAAAAAgOXc+vM9AAAAAED1lbv1V0vb84sMs7Q9AOfHTCkAAAAAAABYjqIUAAAAAAAALEdRCgAAAAAAAJajKAUAAAAAAADLUZQCAAAAAACA5ShKAQAAAAAAwHIUpQAAAAAAAGA5ilIAAAAAAACwHEUpAAAAAAAAWI6iFAAAAAAAACxHUQoAAAAAAACWoygFAAAAAAAAy1GUAgAAAAAAgOUoSgEAAAAAAMByFKUAAAAAAABgOYpSAAAAAAAAsBxFKQAAAAAAAFiOohQAAAAAAAAsR1EKAAAAAAAAlqMoBQAAAAAAAMtRlAIAAAAAAIDlKEoBAAAAAADAchSlAAAAAAAAYDmKUgAAAAAAALAcRSkAAAAAAABYjqIUAAAAAAAALEdRCgAAAAAAAJajKAUAAAAAAADLUZQCAACogn755Rc9+uijCg4OVs2aNXXLLbcoIyPDPG4YhpKSkhQWFiY/Pz917NhR27Ztc7lGXl6ehg4dqtq1a8vf319xcXHav3+/S0xOTo4SEhJkt9tlt9uVkJCgI0eOWNFFAABQyVGUAgAAqGJycnLUvn17eXl56fPPP9f333+vCRMm6NprrzVjxo8fr4kTJ2rKlCnauHGjHA6HunbtqmPHjpkxiYmJWrhwoRYsWKDVq1fr+PHjio2NVWFhoRkTHx+vzMxMpaWlKS0tTZmZmUpISLCyuwAAoJLydHcCAAAAKF+vvfaawsPDNWvWLHNf/fr1zT8bhqHJkydrzJgx6t27tyRpzpw5Cg0N1fz58zVw4EA5nU7NnDlTc+fOVZcuXSRJqampCg8P17Jly9StWzdt375daWlpWrdunaKioiRJM2bMUHR0tHbs2KHGjRtb12kAAFDpMFMKAACgivnkk0/Upk0bPfDAAwoJCVGrVq00Y8YM8/ju3buVlZWlmJgYc5+Pj486dOigNWvWSJIyMjJUUFDgEhMWFqbIyEgzZu3atbLb7WZBSpLatm0ru91uxpwrLy9PR48eddkAAED1RFEKAACgivnpp580bdo0RURE6IsvvtBTTz2lYcOG6Z///KckKSsrS5IUGhrqcl5oaKh5LCsrS97e3goMDDxvTEhISIn2Q0JCzJhzJScnm+tP2e12hYeHX1lnAQBApeXWotRXX32lXr16KSwsTDabTYsWLXI5buUCnHv37lWvXr3k7++v2rVra9iwYcrPz78a3QYAALiqioqKdOutt2rcuHFq1aqVBg4cqP79+2vatGkucTabzeWxYRgl9p3r3JjS4s93ndGjR8vpdJrbvn37LrZbAACginFrUerEiRNq2bKlpkyZUupxqxbgLCwsVM+ePXXixAmtXr1aCxYs0IcffqgRI0Zcvc4DAABcJXXq1FGzZs1c9jVt2lR79+6VJDkcDkkqMZspOzvbnD3lcDiUn5+vnJyc88YcPHiwRPuHDh0qMQurmI+Pj2rVquWyAQCA6smtRakePXrob3/7m7nA5tnOXYAzMjJSc+bM0cmTJzV//nxJMhfgnDBhgrp06aJWrVopNTVVW7Zs0bJlyyTJXIDzH//4h6KjoxUdHa0ZM2Zo8eLF2rFjhyRp6dKl+v7775WamqpWrVqpS5cumjBhgmbMmME6BwAAoNJp3769Oc4ptnPnTtWrV0+S1KBBAzkcDqWnp5vH8/PztWrVKrVr106S1Lp1a3l5ebnEHDhwQFu3bjVjoqOj5XQ6tWHDBjNm/fr1cjqdZgwAAEBZKuyaUlYuwLl27VpFRkYqLCzMjOnWrZvy8vKUkZFxVfsJAABQ3p555hmtW7dO48aN065duzR//ny9++67Gjx4sKQzP7lLTEzUuHHjtHDhQm3dulX9+vVTzZo1FR8fL0my2+164oknNGLECC1fvlzffPONHn30UTVv3ty8G1/Tpk3VvXt39e/fX+vWrdO6devUv39/xcbGcuc9AABwQZ7uTqAs51uAc8+ePWZMeSzAmZWVVaKdwMBAeXt7l7lIp3RmPau8vDzzMbOqAABARXDbbbdp4cKFGj16tF555RU1aNBAkydP1iOPPGLGjBo1Srm5uRo0aJBycnIUFRWlpUuXKiAgwIyZNGmSPD091adPH+Xm5qpz586aPXu2PDw8zJh58+Zp2LBh5peEcXFxZS7NAAAAcLYKW5QqZtUCnJe6SKd05u4xY8eOPW8uAAAA7hAbG6vY2Ngyj9tsNiUlJSkpKanMGF9fX6WkpCglJaXMmKCgIKWmpl5JqgAAoJqqsD/fs3IBTofDUaKdnJwcFRQUlLlIp8TdYwAAAAAAAC5XhS1KWbkAZ3R0tLZu3aoDBw6YMUuXLpWPj49at25dZo7cPQYAAAAAAODyuPXne8ePH9euXbvMx7t371ZmZqaCgoJUt25dcwHOiIgIRUREaNy4cWUuwBkcHKygoCCNHDmyzAU433nnHUnSgAEDXBbgjImJUbNmzZSQkKDXX39dhw8f1siRI9W/f38KTQAAAAAAAFeBW4tSmzZtUqdOnczHw4cPlyT17dtXs2fPtmwBTg8PDy1ZskSDBg1S+/bt5efnp/j4eL3xxhtX+ykAAAAAAAColtxalOrYsaMMwyjzuJULcNatW1eLFy++YM4AAAAAAAC4chV2TSkAAAAAAABUXRSlAAAAAAAAYDmKUgAAAAAAALAcRSkAAAAAAABYjqIUAAAAAAAALEdRCgAAAAAAAJajKAUAAAAAAADLUZQCAAAAAACA5ShKAQAAAAAAwHKXVZRq2LChfv/99xL7jxw5ooYNG15xUgAAAFUN4ycAAABXl1WU+vnnn1VYWFhif15enn755ZcrTgoAAKCqYfwEAADgyvNSgj/55BPzz1988YXsdrv5uLCwUMuXL1f9+vXLLTkAAIDKjvETAABA6S6pKHXPPfdIkmw2m/r27etyzMvLS/Xr19eECRPKLTkAAIDKjvETAABA6S6pKFVUVCRJatCggTZu3KjatWtflaQAAACqCsZPAAAApbukolSx3bt3l3ceAAAAVRrjJwAAAFeXVZSSpOXLl2v58uXKzs42vwEs9t57711xYgAAAFUN4ycAAID/c1lFqbFjx+qVV15RmzZtVKdOHdlstvLOCwAAoEph/AQAAODqsopS06dP1+zZs5WQkFDe+QAAAFRJjJ8AAABc1bick/Lz89WuXbvyzgUAAKDKYvwEAADg6rKKUk8++aTmz59f3rkAAABUWYyfAAAAXF3Wz/dOnTqld999V8uWLVOLFi3k5eXlcnzixInlkhwAAEBVwfgJAADA1WUVpb777jvdcsstkqStW7e6HGPRTgAAgJIYPwEAALi6rKLUypUryzsPAACAKo3xEwAAgKvLWlMKAAAAAAAAuBKXNVOqU6dO551mvmLFistOCAAAoCpi/ARY4/VFGyxt79l7/mBpewBQlVxWUap4PYRiBQUFyszM1NatW9W3b9/yyAsAAKBKYfwEAADg6rKKUpMmTSp1f1JSko4fP35FCQEAAFRFjJ8AAABcleuaUo8++qjee++98rwkAABAlcb4CQAAVFflWpRau3atfH19y/OSAAAAVRrjJwAAUF1d1s/3evfu7fLYMAwdOHBAmzZt0osvvlguiQEAAFQljJ8AAABcXVZRym63uzyuUaOGGjdurFdeeUUxMTHlkhgAAEBVwvgJAADA1WUVpWbNmlXeeQAAAFRpjJ8AAABcXVZRqlhGRoa2b98um82mZs2aqVWrVuWVFwAAQJXE+AkAAOCMyypKZWdn66GHHtKXX36pa6+9VoZhyOl0qlOnTlqwYIGuu+668s4TAACgUmP8BAAA4OqyilJDhw7V0aNHtW3bNjVt2lSS9P3336tv374aNmyY3n///XJNEgAAoLJj/AQAQPk6vOcVS9sLqveSpe1VB5dVlEpLS9OyZcvMAZUkNWvWTG+//TYLdQIAAJSC8RMAAICrGpdzUlFRkby8vErs9/LyUlFR0RUnBQAAUNUwfgIAAHB1WUWpO++8U08//bR+/fVXc98vv/yiZ555Rp07dy635E6fPq2//OUvatCggfz8/NSwYUO98sorLgM3wzCUlJSksLAw+fn5qWPHjtq2bZvLdfLy8jR06FDVrl1b/v7+iouL0/79+11icnJylJCQILvdLrvdroSEBB05cqTc+gIAAKo3q8ZPAAAAlcVlFaWmTJmiY8eOqX79+rrxxht10003qUGDBjp27JhSUlLKLbnXXntN06dP15QpU7R9+3aNHz9er7/+uksb48eP18SJEzVlyhRt3LhRDodDXbt21bFjx8yYxMRELVy4UAsWLNDq1at1/PhxxcbGqrCw0IyJj49XZmam0tLSlJaWpszMTCUkJJRbXwAAQPVm1fgJAACgsrisNaXCw8O1efNmpaen63//+58Mw1CzZs3UpUuXck1u7dq1uvvuu9WzZ09JUv369fX+++9r06ZNks7Mkpo8ebLGjBmj3r17S5LmzJmj0NBQzZ8/XwMHDpTT6dTMmTM1d+5cM7/U1FSFh4dr2bJl6tatm7Zv3660tDStW7dOUVFRkqQZM2YoOjpaO3bsUOPGjcu1XwAAoPqxavwEAABQWVxSUWrFihUaMmSI1q1bp1q1aqlr167q2rWrJMnpdOrmm2/W9OnT9cc//rFckrv99ts1ffp07dy5U40aNdK3336r1atXa/LkyZKk3bt3Kysry2VxUB8fH3Xo0EFr1qzRwIEDlZGRoYKCApeYsLAwRUZGas2aNerWrZvWrl0ru91uFqQkqW3btrLb7VqzZk2ZRam8vDzl5eWZj48ePVou/QYAAFWH1eMnlL/c3OWWt+nnx086AQBV3yX9fG/y5Mnq37+/atWqVeKY3W7XwIEDNXHixHJL7rnnntPDDz+sJk2ayMvLS61atVJiYqIefvhhSVJWVpYkKTQ01OW80NBQ81hWVpa8vb0VGBh43piQkJAS7YeEhJgxpUlOTjbXoLLb7QoPD7/8zgIAgCrJ6vETAABAZXFJRalvv/1W3bt3L/N4TEyMMjIyrjipYh988IFSU1M1f/58bd68WXPmzNEbb7yhOXPmuMTZbDaXx4ZhlNh3rnNjSou/0HVGjx4tp9Npbvv27buYbgEAgGrE6vETAABAZXFJP987ePBgqbcyNi/m6alDhw5dcVLFnn32WT3//PN66KGHJEnNmzfXnj17lJycrL59+8rhcEg6M9OpTp065nnZ2dnm7CmHw6H8/Hzl5OS4zJbKzs5Wu3btzJiDBw+WaP/QoUMlZmGdzcfHRz4+PlfeUQAAUGVZPX4CAACoLC5pptT111+vLVu2lHn8u+++cykOXamTJ0+qRg3XFD08PFRUVCRJatCggRwOh9LT083j+fn5WrVqlVlwat26tby8vFxiDhw4oK1bt5ox0dHRcjqd2rBhgxmzfv16OZ1OMwYAAOByWD1+AgAAqCwuqSh111136aWXXtKpU6dKHMvNzdXLL7+s2NjYckuuV69eevXVV7VkyRL9/PPPWrhwoSZOnKh7771X0pmf3CUmJmrcuHFauHChtm7dqn79+qlmzZqKj4+XdGathieeeEIjRozQ8uXL9c033+jRRx9V8+bNzbvdNG3aVN27d1f//v21bt06rVu3Tv3791dsbCx33gMAAFfE6vETAABAZXFJP9/7y1/+oo8++kiNGjXSkCFD1LhxY9lsNm3fvl1vv/22CgsLNWbMmHJLLiUlRS+++KIGDRqk7OxshYWFaeDAgXrppZfMmFGjRik3N1eDBg1STk6OoqKitHTpUgUEBJgxkyZNkqenp/r06aPc3Fx17txZs2fPloeHhxkzb948DRs2zLxLX1xcnKZMmVJufQEAANWT1eMnAACAyuKSilKhoaFas2aN/vznP2v06NEyDEPSmRlL3bp109SpU8+7BtOlCggI0OTJkzV58uQyY2w2m5KSkpSUlFRmjK+vr1JSUpSSklJmTFBQkFJTU68gWwAAgJKsHj8BAABUFpdUlJKkevXq6bPPPlNOTo527dolwzAUERHhsog4AAAA/g/jJ5Snr1Zut7zNOzo1tbxNAEDVd8lFqWKBgYG67bbbyjMXAACAKo3xEwAAwP+5pIXOAQAAAAAAgPJAUQoAAAAAAACWoygFAAAAAAAAy1GUAgAAAAAAgOUue6FzAAAAAMD5HVo8ytL2rosdb2l7AHAlmCkFAAAAAAAAy1GUAgAAAAAAgOUoSgEAAAAAAMByFKUAAAAAAABgOYpSAAAAAAAAsBxFKQAAAAAAAFiOohQAAAAAAAAsR1EKAAAAAAAAlqMoBQAAAAAAAMtRlAIAAAAAAIDlKEoBAABUccnJybLZbEpMTDT3GYahpKQkhYWFyc/PTx07dtS2bdtczsvLy9PQoUNVu3Zt+fv7Ky4uTvv373eJycnJUUJCgux2u+x2uxISEnTkyBELegUAACo7ilIAAABV2MaNG/Xuu++qRYsWLvvHjx+viRMnasqUKdq4caMcDoe6du2qY8eOmTGJiYlauHChFixYoNWrV+v48eOKjY1VYWGhGRMfH6/MzEylpaUpLS1NmZmZSkhIsKx/AACg8qIoBQAAUEUdP35cjzzyiGbMmKHAwEBzv2EYmjx5ssaMGaPevXsrMjJSc+bM0cmTJzV//nxJktPp1MyZMzVhwgR16dJFrVq1UmpqqrZs2aJly5ZJkrZv3660tDT94x//UHR0tKKjozVjxgwtXrxYO3bscEufAQBA5UFRCgAAoIoaPHiwevbsqS5durjs3717t7KyshQTE2Pu8/HxUYcOHbRmzRpJUkZGhgoKClxiwsLCFBkZacasXbtWdrtdUVFRZkzbtm1lt9vNmHPl5eXp6NGjLhsAAKiePN2dAAAAAMrfggULtHnzZm3cuLHEsaysLElSaGioy/7Q0FDt2bPHjPH29naZYVUcU3x+VlaWQkJCSlw/JCTEjDlXcnKyxo4de+kdAgAAVQ4zpQAAAKqYffv26emnn1Zqaqp8fX3LjLPZbC6PDcMose9c58aUFn++64wePVpOp9Pc9u3bd972AABA1UVRCgAAoIrJyMhQdna2WrduLU9PT3l6emrVqlV666235Onpac6QOnc2U3Z2tnnM4XAoPz9fOTk55405ePBgifYPHTpUYhZWMR8fH9WqVctlAwAA1RM/3wMAAKhiOnfurC1btrjs+9Of/qQmTZroueeeU8OGDeVwOJSenq5WrVpJkvLz87Vq1Sq99tprkqTWrVvLy8tL6enp6tOnjyTpwIED2rp1q8aPHy9Jio6OltPp1IYNG/SHP/xBkrR+/Xo5nU61a9fOqu4CQLnZuXOnpe01atTI0vaAioaiFAAAQBUTEBCgyMhIl33+/v4KDg429ycmJmrcuHGKiIhQRESExo0bp5o1ayo+Pl6SZLfb9cQTT2jEiBEKDg5WUFCQRo4cqebNm5sLpzdt2lTdu3dX//799c4770iSBgwYoNjYWDVu3NjCHgMAgMqIohQAAEA1NGrUKOXm5mrQoEHKyclRVFSUli5dqoCAADNm0qRJ8vT0VJ8+fZSbm6vOnTtr9uzZ8vDwMGPmzZunYcOGmXfpi4uL05QpUyzvDwAAqHwoSgEAAFQDX375pctjm82mpKQkJSUllXmOr6+vUlJSlJKSUmZMUFCQUlNTyylLAABQnbDQOQAAAAAAACxHUQoAAAAAAACWoygFAAAAAAAAy1GUAgAAAAAAgOUoSgEAAAAAAMByFKUAAAAAAABgOYpSAAAAAAAAsBxFKQAAAAAAAFiuwhelfvnlFz366KMKDg5WzZo1dcsttygjI8M8bhiGkpKSFBYWJj8/P3Xs2FHbtm1zuUZeXp6GDh2q2rVry9/fX3Fxcdq/f79LTE5OjhISEmS322W325WQkKAjR45Y0UUAAAAAAIBqp0IXpXJyctS+fXt5eXnp888/1/fff68JEybo2muvNWPGjx+viRMnasqUKdq4caMcDoe6du2qY8eOmTGJiYlauHChFixYoNWrV+v48eOKjY1VYWGhGRMfH6/MzEylpaUpLS1NmZmZSkhIsLK7AAAAAAAA1YanuxM4n9dee03h4eGaNWuWua9+/frmnw3D0OTJkzVmzBj17t1bkjRnzhyFhoZq/vz5GjhwoJxOp2bOnKm5c+eqS5cukqTU1FSFh4dr2bJl6tatm7Zv3660tDStW7dOUVFRkqQZM2YoOjpaO3bsUOPGja3rNAAAAAAAQDVQoWdKffLJJ2rTpo0eeOABhYSEqFWrVpoxY4Z5fPfu3crKylJMTIy5z8fHRx06dNCaNWskSRkZGSooKHCJCQsLU2RkpBmzdu1a2e12syAlSW3btpXdbjdjAAAAAAAAUH4qdFHqp59+0rRp0xQREaEvvvhCTz31lIYNG6Z//vOfkqSsrCxJUmhoqMt5oaGh5rGsrCx5e3srMDDwvDEhISEl2g8JCTFjSpOXl6ejR4+6bAAAAAAAALiwCv3zvaKiIrVp00bjxo2TJLVq1Urbtm3TtGnT9Nhjj5lxNpvN5TzDMErsO9e5MaXFX+g6ycnJGjt27EX1BQAAAAAAAP+nQs+UqlOnjpo1a+ayr2nTptq7d68kyeFwSFKJ2UzZ2dnm7CmHw6H8/Hzl5OScN+bgwYMl2j906FCJWVhnGz16tJxOp7nt27fvEnsIAAAAAABQPVXoolT79u21Y8cOl307d+5UvXr1JEkNGjSQw+FQenq6eTw/P1+rVq1Su3btJEmtW7eWl5eXS8yBAwe0detWMyY6OlpOp1MbNmwwY9avXy+n02nGlMbHx0e1atVy2QAAAAAAAHBhFfrne88884zatWuncePGqU+fPtqwYYPeffddvfvuu5LO/OQuMTFR48aNU0REhCIiIjRu3DjVrFlT8fHxkiS73a4nnnhCI0aMUHBwsIKCgjRy5Eg1b97cvBtf06ZN1b17d/Xv31/vvPOOJGnAgAGKjY3lznsAAAAAAABXQYUuSt12221auHChRo8erVdeeUUNGjTQ5MmT9cgjj5gxo0aNUm5urgYNGqScnBxFRUVp6dKlCggIMGMmTZokT09P9enTR7m5uercubNmz54tDw8PM2bevHkaNmyYeZe+uLg4TZkyxbrOAgAAAAAAVCMVuiglSbGxsYqNjS3zuM1mU1JSkpKSksqM8fX1VUpKilJSUsqMCQoKUmpq6pWkCgAAAAAAgItUodeUAgAAAAAAQNVEUQoAAAAAAACWoygFAAAAAAAAy1GUAgAAAAAAgOUoSgEAAAAAAMByFKUAAAAAAABgOYpSAAAAAAAAsBxFKQAAAAAAAFiOohQAAAAAAAAsR1EKAAAAAAAAlqMoBQAAAAAAAMtRlAIAAAAAAIDlKEoBAAAAAADAchSlAAAAAAAAYDmKUgAAAAAAALAcRSkAAAAAAABYjqIUAAAAAAAALEdRCgAAAAAAAJajKAUAAAAAAADLUZQCAAAAAACA5ShKAQAAAAAAwHIUpQAAAAAAAGA5ilIAAAAAAACwHEUpAAAAAAAAWI6iFAAAAAAAACxHUQoAAAAAAACWoygFAAAAAAAAy1GUAgAAAAAAgOUoSgEAAAAAAMByFKUAAAAAAABgOYpSAAAAAAAAsBxFKQAAAAAAAFiOohQAAAAAAAAsR1EKAAAAAAAAlqMoBQAAAAAAAMtRlAIAAAAAAIDlKlVRKjk5WTabTYmJieY+wzCUlJSksLAw+fn5qWPHjtq2bZvLeXl5eRo6dKhq164tf39/xcXFaf/+/S4xOTk5SkhIkN1ul91uV0JCgo4cOWJBrwAAAAAAAKqfSlOU2rhxo9599121aNHCZf/48eM1ceJETZkyRRs3bpTD4VDXrl117NgxMyYxMVELFy7UggULtHr1ah0/flyxsbEqLCw0Y+Lj45WZmam0tDSlpaUpMzNTCQkJlvUPAAAAAACgOqkURanjx4/rkUce0YwZMxQYGGjuNwxDkydP1pgxY9S7d29FRkZqzpw5OnnypObPny9JcjqdmjlzpiZMmKAuXbqoVatWSk1N1ZYtW7Rs2TJJ0vbt25WWlqZ//OMfio6OVnR0tGbMmKHFixdrx44dbukzAAAAAABAVVYpilKDBw9Wz5491aVLF5f9u3fvVlZWlmJiYsx9Pj4+6tChg9asWSNJysjIUEFBgUtMWFiYIiMjzZi1a9fKbrcrKirKjGnbtq3sdrsZAwAAAAAAgPLj6e4ELmTBggXavHmzNm7cWOJYVlaWJCk0NNRlf2hoqPbs2WPGeHt7u8ywKo4pPj8rK0shISElrh8SEmLGlCYvL095eXnm46NHj15krwAAAAAAAKq3Cj1Tat++fXr66aeVmpoqX1/fMuNsNpvLY8MwSuw717kxpcVf6DrJycnmwuh2u13h4eHnbRMAAAAAAABnVOiiVEZGhrKzs9W6dWt5enrK09NTq1at0ltvvSVPT09zhtS5s5mys7PNYw6HQ/n5+crJyTlvzMGDB0u0f+jQoRKzsM42evRoOZ1Oc9u3b98V9RcAAAAAAKC6qNBFqc6dO2vLli3KzMw0tzZt2uiRRx5RZmamGjZsKIfDofT0dPOc/Px8rVq1Su3atZMktW7dWl5eXi4xBw4c0NatW82Y6OhoOZ1ObdiwwYxZv369nE6nGVMaHx8f1apVy2UDAAAAAADAhVXoNaUCAgIUGRnpss/f31/BwcHm/sTERI0bN04RERGKiIjQuHHjVLNmTcXHx0uS7Ha7nnjiCY0YMULBwcEKCgrSyJEj1bx5c3Ph9KZNm6p79+7q37+/3nnnHUnSgAEDFBsbq8aNG1vYYwAAAAAAgOqhQhelLsaoUaOUm5urQYMGKScnR1FRUVq6dKkCAgLMmEmTJsnT01N9+vRRbm6uOnfurNmzZ8vDw8OMmTdvnoYNG2bepS8uLk5TpkyxvD8AAAAAAADVQaUrSn355Zcuj202m5KSkpSUlFTmOb6+vkpJSVFKSkqZMUFBQUpNTS2nLAEAAAAAAHA+FXpNKQAAAAAAAFRNFKUAAAAAAABgOYpSAAAAAAAAsBxFKQAAgComOTlZt912mwICAhQSEqJ77rlHO3bscIkxDENJSUkKCwuTn5+fOnbsqG3btrnE5OXlaejQoapdu7b8/f0VFxen/fv3u8Tk5OQoISFBdrtddrtdCQkJOnLkyNXuIgAAqAIoSgEAAFQxq1at0uDBg7Vu3Tqlp6fr9OnTiomJ0YkTJ8yY8ePHa+LEiZoyZYo2btwoh8Ohrl276tixY2ZMYmKiFi5cqAULFmj16tU6fvy4YmNjVVhYaMbEx8crMzNTaWlpSktLU2ZmphISEiztLwAAqJwq3d33AAAAcH5paWkuj2fNmqWQkBBlZGTojjvukGEYmjx5ssaMGaPevXtLkubMmaPQ0FDNnz9fAwcOlNPp1MyZMzV37lx16dJFkpSamqrw8HAtW7ZM3bp10/bt25WWlqZ169YpKipKkjRjxgxFR0drx44daty4sbUdBwAAlQozpQAAAKo4p9MpSQoKCpIk7d69W1lZWYqJiTFjfHx81KFDB61Zs0aSlJGRoYKCApeYsLAwRUZGmjFr166V3W43C1KS1LZtW9ntdjMGAACgLMyUAgAAqMIMw9Dw4cN1++23KzIyUpKUlZUlSQoNDXWJDQ0N1Z49e8wYb29vBQYGlogpPj8rK0shISEl2gwJCTFjzpWXl6e8vDzz8dGjRy+zZwAAoLJjphQAAEAVNmTIEH333Xd6//33Sxyz2Wwujw3DKLHvXOfGlBZ/vuskJyebi6Lb7XaFh4dfTDcAAEAVRFEKAACgiho6dKg++eQTrVy5UjfccIO53+FwSFKJ2UzZ2dnm7CmHw6H8/Hzl5OScN+bgwYMl2j106FCJWVjFRo8eLafTaW779u27/A4CAIBKjaIUAABAFWMYhoYMGaKPPvpIK1asUIMGDVyON2jQQA6HQ+np6ea+/Px8rVq1Su3atZMktW7dWl5eXi4xBw4c0NatW82Y6OhoOZ1ObdiwwYxZv369nE6nGXMuHx8f1apVy2UDAADVE2tKAQAAVDGDBw/W/Pnz9fHHHysgIMCcEWW32+Xn5yebzabExESNGzdOERERioiI0Lhx41SzZk3Fx8ebsU888YRGjBih4OBgBQUFaeTIkWrevLl5N76mTZuqe/fu6t+/v9555x1J0oABAxQbG8ud9wAAwAVRlAIAAKhipk2bJknq2LGjy/5Zs2apX79+kqRRo0YpNzdXgwYNUk5OjqKiorR06VIFBASY8ZMmTZKnp6f69Omj3Nxcde7cWbNnz5aHh4cZM2/ePA0bNsy8S19cXJymTJlydTsIAACqBIpSAAAAVYxhGBeMsdlsSkpKUlJSUpkxvr6+SklJUUpKSpkxQUFBSk1NvZw0AQBANceaUgAAAAAAALAcRSkAAAAAAABYjqIUAAAAAAAALEdRCgAAAAAAAJajKAUAAAAAAADLUZQCAAAAAACA5ShKAQAAAAAAwHIUpQAAAAAAAGA5ilIAAAAAAACwHEUpAAAAAAAAWI6iFAAAAAAAACxHUQoAAAAAAACWoygFAAAAAAAAy1GUAgAAAAAAgOUoSgEAAAAAAMByFKUAAAAAAABgOYpSAAAAAAAAsBxFKQAAAAAAAFiOohQAAAAAAAAsR1EKAAAAAAAAlqMoBQAAAAAAAMtRlAIAAAAAAIDlKEoBAAAAAADAchW6KJWcnKzbbrtNAQEBCgkJ0T333KMdO3a4xBiGoaSkJIWFhcnPz08dO3bUtm3bXGLy8vI0dOhQ1a5dW/7+/oqLi9P+/ftdYnJycpSQkCC73S673a6EhAQdOXLkancRAAAAAACgWqrQRalVq1Zp8ODBWrdundLT03X69GnFxMToxIkTZsz48eM1ceJETZkyRRs3bpTD4VDXrl117NgxMyYxMVELFy7UggULtHr1ah0/flyxsbEqLCw0Y+Lj45WZmam0tDSlpaUpMzNTCQkJlvYXAAAAAACguvB0dwLnk5aW5vJ41qxZCgkJUUZGhu644w4ZhqHJkydrzJgx6t27tyRpzpw5Cg0N1fz58zVw4EA5nU7NnDlTc+fOVZcuXSRJqampCg8P17Jly9StWzdt375daWlpWrdunaKioiRJM2bMUHR0tHbs2KHGjRtb23EAAAAAAIAqrkLPlDqX0+mUJAUFBUmSdu/eraysLMXExJgxPj4+6tChg9asWSNJysjIUEFBgUtMWFiYIiMjzZi1a9fKbrebBSlJatu2rex2uxlTmry8PB09etRlAwAAAAAAwIVVmqKUYRgaPny4br/9dkVGRkqSsrKyJEmhoaEusaGhoeaxrKwseXt7KzAw8LwxISEhJdoMCQkxY0qTnJxsrkFlt9sVHh5++R0EAAAAAACoRipNUWrIkCH67rvv9P7775c4ZrPZXB4bhlFi37nOjSkt/kLXGT16tJxOp7nt27fvQt0AAAAAAACAKklRaujQofrkk0+0cuVK3XDDDeZ+h8MhSSVmM2VnZ5uzpxwOh/Lz85WTk3PemIMHD5Zo99ChQyVmYZ3Nx8dHtWrVctkAAAAAAABwYRW6KGUYhoYMGaKPPvpIK1asUIMGDVyON2jQQA6HQ+np6ea+/Px8rVq1Su3atZMktW7dWl5eXi4xBw4c0NatW82Y6OhoOZ1ObdiwwYxZv369nE6nGQMAAAAAAIDyU6Hvvjd48GDNnz9fH3/8sQICAswZUXa7XX5+frLZbEpMTNS4ceMUERGhiIgIjRs3TjVr1lR8fLwZ+8QTT2jEiBEKDg5WUFCQRo4cqebNm5t342vatKm6d++u/v3765133pEkDRgwQLGxsdx5DwAAAAAA4Cqo0EWpadOmSZI6duzosn/WrFnq16+fJGnUqFHKzc3VoEGDlJOTo6ioKC1dulQBAQFm/KRJk+Tp6ak+ffooNzdXnTt31uzZs+Xh4WHGzJs3T8OGDTPv0hcXF6cpU6Zc3Q4CAAAAAABUUxW6KGUYxgVjbDabkpKSlJSUVGaMr6+vUlJSlJKSUmZMUFCQUlNTLydNAAAAAAAAXKIKvaYUAAAAAAAAqiaKUgAAAAAAALAcRSkAAAAAAABYjqIUAAAAAAAALEdRCgAAAAAAAJajKAUAAAAAAADLUZQCAAAAAACA5ShKAQAAAAAAwHIUpQAAAAAAAGA5ilIAAAAAAACwHEUpAAAAAAAAWI6iFAAAAAAAACxHUQoAAAAAAACWoygFAAAAAAAAy1GUAgAAAAAAgOUoSgEAAAAAAMByFKUAAAAAAABgOYpSAAAAAAAAsBxFKQAAAAAAAFiOohQAAAAAAAAsR1EKAAAAAAAAlvN0dwIAAAAAAGssyhxtaXv33JJsaXsAKhdmSgEAAAAAAMByFKUAAAAAAABgOYpSAAAAAAAAsBxrSgEAAAC4bIcm/c3yNq975i+WtwkAKH/MlAIAAAAAAIDlKEoBAAAAAADAchSlAAAAAAAAYDmKUgAAAAAAALAcRSkAAAAAAABYjqIUAAAAAAAALEdRCgAAAAAAAJajKAUAAAAAAADLUZQCAAAAAACA5ShKAQAAAAAAwHIUpc4xdepUNWjQQL6+vmrdurW+/vprd6cEAABQoTF+AgAAl4Oi1Fk++OADJSYmasyYMfrmm2/0xz/+UT169NDevXvdnRoAAECFxPgJAABcLopSZ5k4caKeeOIJPfnkk2ratKkmT56s8PBwTZs2zd2pAQAAVEiMnwAAwOWiKPX/5efnKyMjQzExMS77Y2JitGbNGjdlBQAAUHExfgIAAFfC090JVBS//fabCgsLFRoa6rI/NDRUWVlZpZ6Tl5envLw887HT6ZQkHT169ILt5Z3MvYJscTHP8aU4lpt34SCcl285vya5uSfK9XrVUfn/PeE1uVLl/Zrknsgv1+tVNxf7ehTHGYZxNdOplK7W+Cn3uPWfN+d7P7jj36SCgrLzOXHiuIWZnHG+5+fYqVMWZnKGz3nyOXXS2ufnQp8lx05aO84833MjSSePW5vPhZ6f3OPHLMrkjIIL5HP8eMV6/5w8ddKiTM64UD7Hj1n8/PifP5+jx6z9/PG84PvZ2n8vLvj3y+J/v87+t+tix08Upc5hs9lcHhuGUWJfseTkZI0dO7bE/vDw8KuSG/7P23rK3SngXCPecncGOMfIAe7OACWMcHcCONtAzbmk+GPHjslut1+lbCq3qjB+4q9nJfPCq+7OwPSSuxMooaKNySa5OwFciiR3J1DRJbs7AReV4d+uC42fKEr9f7Vr15aHh0eJb/Wys7NLfPtXbPTo0Ro+fLj5uKioSIcPH1ZwcHCZA7HK4OjRowoPD9e+fftUq1Ytd6cD8ZpURLwmFQuvR8VTlV4TwzB07NgxhYWFuTuVCqcijZ8q2nuOfMiHfMiHfMinOudzseMnilL/n7e3t1q3bq309HTde++95v709HTdfffdpZ7j4+MjHx8fl33XXnvt1UzTUrVq1aoQfynwf3hNKh5ek4qF16PiqSqvCTOkSlcRx08V7T1HPudHPudHPudHPudHPudHPudXHvlczPiJotRZhg8froSEBLVp00bR0dF69913tXfvXj31FD8VAwAAKA3jJwAAcLkoSp3lwQcf1O+//65XXnlFBw4cUGRkpD777DPVq1fP3akBAABUSIyfAADA5aIodY5BgwZp0KBB7k7DrXx8fPTyyy+XmFoP9+E1qXh4TSoWXo+Kh9ekeqkI46eK9p4jn/Mjn/Mjn/Mjn/Mjn/Mjn/OzOh+bwf2NAQAAAAAAYLEa7k4AAAAAAAAA1Q9FKQAAAAAAAFiOohQAAAAAAAAsR1EKpq+++kq9evVSWFiYbDabFi1a5O6UqrXk5GTddtttCggIUEhIiO655x7t2LHD3WlVa9OmTVOLFi1Uq1Yt1apVS9HR0fr888/dnRbOkpycLJvNpsTERHenUm0lJSXJZrO5bA6Hw91poRqYOnWqGjRoIF9fX7Vu3Vpff/21W/KoaOOpijyeqAif2RXxM+uXX37Ro48+quDgYNWsWVO33HKLMjIy3JJL/fr1Szw/NptNgwcPdks+p0+f1l/+8hc1aNBAfn5+atiwoV555RUVFRW5JZ9jx44pMTFR9erVk5+fn9q1a6eNGzda1v6FPm8Mw1BSUpLCwsLk5+enjh07atu2bW7L56OPPlK3bt1Uu3Zt2Ww2ZWZmXrVcLpRPQUGBnnvuOTVv3lz+/v4KCwvTY489pl9//dUt+UhnPo+aNGkif39/BQYGqkuXLlq/fr3b8jnbwIEDZbPZNHny5HLPg6IUTCdOnFDLli01ZcoUd6cCSatWrdLgwYO1bt06paen6/Tp04qJidGJEyfcnVq1dcMNN+jvf/+7Nm3apE2bNunOO+/U3XfffVX/ccfF27hxo9599121aNHC3alUezfffLMOHDhgblu2bHF3SqjiPvjgAyUmJmrMmDH65ptv9Mc//lE9evTQ3r17Lc+loo2nKup4oiJ9Zlekz6ycnBy1b99eXl5e+vzzz/X9999rwoQJuvbaa92Sz8aNG12em/T0dEnSAw884JZ8XnvtNU2fPl1TpkzR9u3bNX78eL3++utKSUlxSz5PPvmk0tPTNXfuXG3ZskUxMTHq0qWLfvnlF0vav9Dnzfjx4zVx4kRNmTJFGzdulMPhUNeuXXXs2DG35HPixAm1b99ef//7369K+5eSz8mTJ7V582a9+OKL2rx5sz766CPt3LlTcXFxbslHkho1aqQpU6Zoy5YtWr16terXr6+YmBgdOnTILfkUW7RokdavX6+wsLCrkocMoBSSjIULF7o7DZwlOzvbkGSsWrXK3angLIGBgcY//vEPd6dR7R07dsyIiIgw0tPTjQ4dOhhPP/20u1Oqtl5++WWjZcuW7k4D1cwf/vAH46mnnnLZ16RJE+P55593U0ZnVMTxVEUYT1Skz+yK9pn13HPPGbfffru70yjT008/bdx4441GUVGRW9rv2bOn8fjjj7vs6927t/Hoo49ansvJkycNDw8PY/HixS77W7ZsaYwZM8byfM79vCkqKjIcDofx97//3dx36tQpw263G9OnT7c8n7Pt3r3bkGR88803Vz2Pi8mn2IYNGwxJxp49eypEPk6n05BkLFu2zG357N+/37j++uuNrVu3GvXq1TMmTZpU7m0zUwqoJJxOpyQpKCjIzZlAkgoLC7VgwQKdOHFC0dHR7k6n2hs8eLB69uypLl26uDsVSPrhhx8UFhamBg0a6KGHHtJPP/3k7pRQheXn5ysjI0MxMTEu+2NiYrRmzRo3ZVVxVYTxREX7zK5In1mffPKJ2rRpowceeEAhISFq1aqVZsyY4bZ8zpafn6/U1FQ9/vjjstlsbsnh9ttv1/Lly7Vz505J0rfffqvVq1frrrvusjyX06dPq7CwUL6+vi77/fz8tHr1asvzOdfu3buVlZXl8tno4+OjDh068NlYBqfTKZvN5raZiWfLz8/Xu+++K7vdrpYtW7olh6KiIiUkJOjZZ5/VzTfffNXa8bxqVwZQbgzD0PDhw3X77bcrMjLS3elUa1u2bFF0dLROnTqla665RgsXLlSzZs3cnVa1tmDBAm3evNnSNRxQtqioKP3zn/9Uo0aNdPDgQf3tb39Tu3bttG3bNgUHB7s7PVRBv/32mwoLCxUaGuqyPzQ0VFlZWW7KqmKqCOOJivaZXdE+s3766SdNmzZNw4cP1wsvvKANGzZo2LBh8vHx0WOPPWZ5PmdbtGiRjhw5on79+rkth+eee05Op1NNmjSRh4eHCgsL9eqrr+rhhx+2PJeAgABFR0frr3/9q5o2barQ0FC9//77Wr9+vSIiIizP51zFn3+lfTbu2bPHHSlVaKdOndLzzz+v+Ph41apVy215LF68WA899JBOnjypOnXqKD09XbVr13ZLLq+99po8PT01bNiwq9oORSmgEhgyZIi+++67CvGtS3XXuHFjZWZm6siRI/rwww/Vt29frVq1isKUm+zbt09PP/20li5dWuKbSrhHjx49zD83b95c0dHRuvHGGzVnzhwNHz7cjZmhqjt35oZhGG6bzVFRuXs8URE/syvaZ1ZRUZHatGmjcePGSZJatWqlbdu2adq0aW4vSs2cOVM9evS4euvKXIQPPvhAqampmj9/vm6++WZlZmYqMTFRYWFh6tu3r+X5zJ07V48//riuv/56eXh46NZbb1V8fLw2b95seS5l4bPxwgoKCvTQQw+pqKhIU6dOdWsunTp1UmZmpn777TfNmDFDffr00fr16xUSEmJpHhkZGXrzzTe1efPmq/5+4ed7QAU3dOhQffLJJ1q5cqVuuOEGd6dT7Xl7e+umm25SmzZtlJycrJYtW+rNN990d1rVVkZGhrKzs9W6dWt5enrK09NTq1at0ltvvSVPT08VFha6O8Vqz9/fX82bN9cPP/zg7lRQRdWuXVseHh4lZkVlZ2eXmCFQnVWE8URl+Mx292dWnTp1SnzR1bRpU7cs2n+2PXv2aNmyZXryySfdmsezzz6r559/Xg899JCaN2+uhIQEPfPMM0pOTnZLPjfeeKNWrVql48ePa9++fdqwYYMKCgrUoEEDt+RztuK7SPLZeH4FBQXq06ePdu/erfT0dLfOkpLOfAbddNNNatu2rWbOnClPT0/NnDnT8jy+/vprZWdnq27duubn9Z49ezRixAjVr1+/XNuiKAVUUIZhaMiQIfroo4+0YsWKCvGPG0oyDEN5eXnuTqPa6ty5s7Zs2aLMzExza9OmjR555BFlZmbKw8PD3SlWe3l5edq+fbvq1Knj7lRQRXl7e6t169bmXcGKpaenq127dm7KquKoSOOJyvCZ7e7PrPbt22vHjh0u+3bu3Kl69eq5JZ9is2bNUkhIiHr27OnWPE6ePKkaNVz/C+vh4aGioiI3ZXSGv7+/6tSpo5ycHH3xxRe6++673ZqPJDVo0EAOh8PlszE/P1+rVq3is/H/Ky5I/fDDD1q2bFmFXGbAXf/XSEhI0HfffefyeR0WFqZnn31WX3zxRbm2xc/3YDp+/Lh27dplPt69e7cyMzMVFBSkunXrujGz6mnw4MGaP3++Pv74YwUEBJjfctjtdvn5+bk5u+rphRdeUI8ePRQeHq5jx45pwYIF+vLLL5WWlubu1KqtgICAEuui+Pv7Kzg4mPXX3GTkyJHq1auX6tatq+zsbP3tb3/T0aNH3fKzClQfw4cPV0JCgtq0aaPo6Gi9++672rt3r5566inLc6lo46mKNJ6oiJ/ZFe0z65lnnlG7du00btw49enTRxs2bNC7776rd9991y35SGd+Ujhr1iz17dtXnp7u/e9jr1699Oqrr6pu3bq6+eab9c0332jixIl6/PHH3ZLPF198IcMw1LhxY+3atUvPPvusGjdurD/96U+WtH+hz5vExESNGzdOERERioiI0Lhx41SzZk3Fx8e7JZ/Dhw9r7969+vXXXyXJLMA6HA5zZpdV+YSFhen+++/X5s2btXjxYhUWFpqfj0FBQfL29rY0n+DgYL366quKi4tTnTp19Pvvv2vq1Knav3+/HnjggXLP5UL51K1bt0SRzsvLSw6HQ40bNy7fRMr9fn6otFauXGlIKrH17dvX3alVS6W9FpKMWbNmuTu1auvxxx836tWrZ3h7exvXXXed0blzZ2Pp0qXuTgvncPftxau7Bx980KhTp47h5eVlhIWFGb179za2bdvm7rRQDbz99tvmZ/Stt95qrFq1yi15VLTxVEUfT7j7M7sifmZ9+umnRmRkpOHj42M0adLEePfdd92azxdffGFIMnbs2OHWPAzDMI4ePWo8/fTTRt26dQ1fX1+jYcOGxpgxY4y8vDy35PPBBx8YDRs2NLy9vQ2Hw2EMHjzYOHLkiGXtX+jzpqioyHj55ZcNh8Nh+Pj4GHfccYexZcsWt+Uza9asUo+//PLLlueze/fuMj8fV65caXk+ubm5xr333muEhYUZ3t7eRp06dYy4uDhjw4YNVyWXC+VTmnr16hmTJk0q9zxshmEY5VTfAgAAAAAAAC4Ka0oBAAAAAADAchSlAAAAAAAAYDmKUgAAAAAAALAcRSkAAAAAAABYjqIUAAAAAAAALEdRCgAAAAAAAJajKAUAAAAAAADLUZQCAAAAAACA5ShKAQAAAAAAwHIUpQBcMZvNpkWLFrmt/R07dsjhcOjYsWNuy+Fc/fr10z333OPuNCwzZcoUxcXFuTsNAABwhTp27KjExMQS+xctWiSbzWZ5PueOqfr16yebzSabzSYvLy+Fhoaqa9eueu+991RUVGR5fgCuDEUpAOeVlZWloUOHqmHDhvLx8VF4eLh69eql5cuXuzs105gxYzR48GAFBARIkr788kvZbDYdOXKkRGz9+vU1efJkaxMsRXGOxdt1112nHj166Ntvvy2X65c1oLxa+vfvr40bN2r16tWWtQkAAKqO/Pz8i47t3r27Dhw4oJ9//lmff/65OnXqpKefflqxsbE6ffr0VcwSQHmjKAWgTD///LNat26tFStWaPz48dqyZYvS0tLUqVMnDR482N3pSZL279+vTz75RH/605/cncpl2bFjhw4cOKAlS5YoJydH3bt3l9PpdHdapoKCgouK8/HxUXx8vFJSUq5yRgAAoCL49ttv1alTJwUEBKhWrVpq3bq1Nm3aZB5fs2aN7rjjDvn5+Sk8PFzDhg3TiRMnzOP169fX3/72N/Xr1092u139+/e/6LZ9fHzkcDh0/fXX69Zbb9ULL7ygjz/+WJ9//rlmz55dnt0EcJVRlAJQpkGDBslms2nDhg26//771ahRI918880aPny41q1bV+Z5zz33nBo1aqSaNWuqYcOGevHFF12KG+cbxOzZs0e9evVSYGCg/P39dfPNN+uzzz4rs61//etfatmypW644YbL6uPEiRPVvHlz+fv7Kzw8XIMGDdLx48fN47Nnz9a1116rL774Qk2bNtU111xjfjtXrLCwUMOHD9e1116r4OBgjRo1SoZhXFT7ISEhcjgc+sMf/qAJEyYoKyvLfG4//PBD3XzzzfLx8VH9+vU1YcIEl3OnTp2qiIgI+fr6KjQ0VPfff7+kM9PaV61apTfffNOcifXzzz+bfTnbuVPxk5KSdMstt+i9994zZ8cZhiGn06kBAwYoJCREtWrV0p133lliVldcXJwWLVqk3Nzci+o7AACovB555BHdcMMN2rhxozIyMvT888/Ly8tLkrRlyxZ169ZNvXv31nfffacPPvhAq1ev1pAhQ1yu8frrrysyMlIZGRl68cUXryifO++8Uy1bttRHH310RdcBYC1PdycAoGI6fPiw0tLS9Oqrr8rf37/E8XOLG2cLCAjQ7NmzFRYWpi1btqh///4KCAjQqFGjJJ0ZxLRq1UrTpk2Th4eHMjMzzUHM4MGDlZ+fr6+++kr+/v76/vvvdc0115TZ1ldffaU2bdpcdj9r1Kiht956S/Xr19fu3bs1aNAgjRo1SlOnTjVjTp48qTfeeENz585VjRo19Oijj2rkyJGaN2+eJGnChAl67733NHPmTDVr1kwTJkzQwoULdeedd15SLn5+fpLOzE7KyMhQnz59lJSUpAcffFBr1qzRoEGDFBwcrH79+mnTpk0aNmyY5s6dq3bt2unw4cP6+uuvJUlvvvmmdu7cqcjISL3yyiuSpOuuu+6i89i1a5f+9a9/6cMPP5SHh4ckqWfPngoKCtJnn30mu92ud955R507d9bOnTsVFBQkSWrTpo0KCgq0YcMGdejQ4ZL6DgAAKpe9e/fq2WefVZMmTSRJERER5rHXX39d8fHx5lICEREReuutt9ShQwdNmzZNvr6+ks4UkkaOHFluOTVp0kTfffdduV0PwNVHUQpAqXbt2iXDMMyBxqX4y1/+Yv65fv36GjFihD744AOzKHW+QczevXt13333qXnz5pKkhg0bnret4p8Ylqa02VMnT550eXz2uksNGjTQX//6V/35z392KUoVFBRo+vTpuvHGGyVJQ4YMMYs9kjR58mSNHj1a9913nyRp+vTp+uKLL86b97l+//13jR07VgEBAfrDH/6gZ555Rp07dza/NWzUqJG+//57vf766+rXr5/27t0rf39/xcbGKiAgQPXq1VOrVq0kSXa7Xd7e3qpZs6YcDscl5SGdWdNh7ty5ZiFrxYoV2rJli7Kzs+Xj4yNJeuONN7Ro0SL95z//0YABAyRJ/v7+uvbaa/Xzzz9TlAIAoIobPny4nnzySc2dO1ddunTRAw88YI6VMjIytGvXLvMLPEkyDENFRUXavXu3mjZtKklX9MViaQzDcMti7AAuH0UpAKUq/vnZ5fzD/p///EeTJ0/Wrl27dPz4cZ0+fVq1atUyj59vEDNs2DD9+c9/1tKlS9WlSxfdd999atGiRZlt5ebmmt+2nevrr782Fz8v1rFjR5fHK1eu1Lhx4/T999/r6NGjOn36tE6dOqUTJ06YM8Rq1qxp5idJderUUXZ2tiTJ6XTqwIEDio6ONo97enqqTZs2F/UTvuLC2YkTJxQREaF///vfCgkJ0fbt23X33Xe7xLZv316TJ09WYWGhunbtqnr16qlhw4bq3r27unfvrnvvvVc1a9a8YJsXUq9ePZeZVRkZGTp+/LiCg4Nd4nJzc/Xjjz+67PPz8ytR+AMAAJVHrVq1Sl3f8siRIy7juaSkJMXHx2vJkiX6/PPP9fLLL2vBggW69957VVRUpIEDB2rYsGElrlO3bl3zz6XNxr8S27dvV4MGDcr1mgCuLtaUAlCqiIgI2Ww2bd++/ZLOW7dunR566CH16NFDixcv1jfffKMxY8a43FElKSlJ27ZtU8+ePbVixQo1a9ZMCxculCQ9+eST+umnn5SQkKAtW7aoTZs25108u3bt2srJySn1WIMGDXTTTTe5bJ6e/1eL37Nnj+666y5FRkbqww8/VEZGht5++21Jrgt8F/+0sJjNZrvoNaMu5Ouvv9a3334rp9OpnTt3qlu3bpJK/6bv7DYDAgK0efNmvf/++6pTp45eeukltWzZstQ7DharUaNGibxLW8j83AFiUVGR6tSpo8zMTJdtx44devbZZ11iDx8+fEk/FQQAABVLkyZNXBYsL7Zx40Y1btzYZV+jRo30zDPPaOnSperdu7dmzZolSbr11lu1bdu2EuOwm266Sd7e3lcl7+KZ3cUz1wFUDhSlAJQqKChI3bp109tvv+1yp5RiZRU//vvf/6pevXoaM2aM2rRpo4iICO3Zs6dEXFmDGEkKDw/XU089pY8++kgjRozQjBkzysyzVatW+v777y+9g5I2bdqk06dPa8KECWrbtq0aNWqkX3/99ZKuYbfbVadOHZeF30+fPq2MjIyLOr9Bgwa68cYbXb55lKRmzZpp9erVLvvWrFmjRo0ames8eXp6qkuXLho/fry+++47/fzzz1qxYoUkydvbW4WFhS7nX3fddTp27JjL65mZmXnBHG+99VZlZWXJ09OzxMCydu3aZtyPP/6oU6dOmT8jBAAAlc+gQYP0448/avDgwfr222+1c+dOvf3225o5c6b5ZVRubq6GDBmiL7/8Unv27NF///tfbdy40fxZ3nPPPae1a9dq8ODByszM1A8//KBPPvlEQ4cOLZcc8/LylJWVpV9++UWbN2/WuHHjdPfddys2NlaPPfZYubQBwBr8fA9AmaZOnap27drpD3/4g1555RW1aNFCp0+fVnp6uqZNm1bqLKqbbrpJe/fu1YIFC3TbbbdpyZIl5iwo6cwg5tlnn9X999+vBg0aaP/+/dq4caP5rVZiYqJ69OihRo0aKScnRytWrDAHOKXp1q2bnnzySRUWFprFmot144036vTp00pJSVGvXr303//+V9OnT7+ka0jS008/rb///e+KiIhQ06ZNNXHixPPOWLoYI0aM0G233aa//vWvevDBB7V27VpNmTLFXOtq8eLF+umnn3THHXcoMDBQn332mYqKisxvMOvXr6/169fr559/1jXXXKOgoCBFRUWpZs2aeuGFFzR06FBt2LDhom6b3KVLF0VHR+uee+7Ra6+9psaNG+vXX3/VZ599pnvuucdcD+Lrr79Ww4YNXX7qCAAAKpf69evr66+/1pgxYxQTE6NTp06pUaNGmj17th544AFJkoeHh37//Xc99thjOnjwoGrXrq3evXtr7NixkqQWLVpo1apVGjNmjP74xz/KMAzdeOONevDBB8slx7S0NNWpU0eenp4KDAxUy5Yt9dZbb6lv376qUYN5F0ClYgDAefz666/G4MGDjXr16hne3t7G9ddfb8TFxRkrV640YyQZCxcuNB8/++yzRnBwsHHNNdcYDz74oDFp0iTDbrcbhmEYeXl5xkMPPWSEh4cb3t7eRlhYmDFkyBAjNzfXMAzDGDJkiHHjjTcaPj4+xnXXXWckJCQYv/32W5n5nT592rj++uuNtLQ0c9/KlSsNSUZOTk6J+Hr16hmTJk0yH0+cONGoU6eO4efnZ3Tr1s345z//6XLurFmzzNyLLVy40Dj747OgoMB4+umnjVr/r737VVE0CuMA/G0Q/MqoVdBkMmi1GQST4AUIRpvFGzAa7AajTbB4Dd6CRYtgsAp6Ae+mlV2GWXZhOMPA89T3hN+pP86ft7col8sxm81iPB7HcDj8MPffMv6y2+2i2WxGoVCIer0ey+XyNTscDtHtdqNSqUSe59FqtWK73b7mp9MpOp1O5HkeWZbF5XJ5ZW80GlEsFmMwGMR6vf5jL/P5PNrt9rssj8cjptNpVKvVKBQKUavVYjQaxfV6fa3p9/uxWCw+3A8AAMDvfkR80sMoAF9ktVpl+/3+v3+84/Mcj8es1+tl5/M5K5VKXx0HAAD4BlzfA769yWSS3e/37Pl8vvttjzRut1u22WwUUgAAwD9zUgoAAACA5LwCBwAAAEBySikAAAAAklNKAQAAAJCcUgoAAACA5JRSAAAAACSnlAIAAAAgOaUUAAAAAMkppQAAAABITikFAAAAQHI/AfneelDTeyL9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add your code below\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# read file\n",
    "df = pd.read_csv(\"data/Postures.csv\")\n",
    "# convert ? to NA, because there are many '?' exist in the dataset\n",
    "df = df.replace('?', np.nan)\n",
    "df.dropna(inplace=True, axis=1) # since drop all columns, set axis=1\n",
    "df = df.drop(index=0) # because the 1st row is all 0\n",
    "print(df.head())\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# --- Plot 1: Class distribution ---\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='Class', hue='Class', data=df, palette='Set2', legend=False)\n",
    "plt.title('Distribution of Hand Posture Classes')\n",
    "plt.xlabel('Class (Hand Posture)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# --- Plot 2: User distribution ---\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x='User', hue='User', data=df, palette='Set3', legend=False)\n",
    "plt.title('Distribution of Users')\n",
    "plt.xlabel('User ID')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d54c4-f02c-4c4a-9756-19d25c899209",
   "metadata": {},
   "source": [
    "## Problem 3a - basic splitting (5 points)\n",
    "\n",
    "Create shuffled train/validation/test sets (60/20/20 ratio) and use `linear_ML_pipeline` to predict the class. \n",
    "Print out the best hyperparameter values, and the test score.\n",
    "Repeat this process with 5 random states and report the mean and stdev of the test score.\n",
    "\n",
    "You may receive some warnings about models failing to converge. This usually happens when C is too high (aka not enough regularization). Play around with those parameters in `linear_ML_pipeline` until you no longer see that warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1963af1c-4cdd-49d4-8347-faff8cb7d7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameter and Test Accuracy: C=10.000, l1_ratio=0.10, test accuracy=0.5676\n",
      "Best Hyperparameter and Test Accuracy: C=10.000, l1_ratio=0.10, test accuracy=0.5765\n",
      "Best Hyperparameter and Test Accuracy: C=10.000, l1_ratio=0.78, test accuracy=0.5683\n",
      "Best Hyperparameter and Test Accuracy: C=10.000, l1_ratio=0.10, test accuracy=0.5694\n",
      "Best Hyperparameter and Test Accuracy: C=10.000, l1_ratio=0.33, test accuracy=0.5760\n",
      "\n",
      "Mean test accuracy = 0.5716, std = 0.0039\n"
     ]
    }
   ],
   "source": [
    "# add your code here\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Identify features (X) and target (y)\n",
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']\n",
    "\n",
    "test_scores = []\n",
    "# Loop over 5 random states\n",
    "for seed in range(5):\n",
    "    # Shuffle and Split data set: 60% train, 20% val, 20% test\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=0.6, shuffle=True, random_state=seed)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size=0.5, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Call the linear_ML_pipeline\n",
    "    best_model, best_params, test_score = linear_ML_pipeline(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        is_classif=True,\n",
    "        continuous_ftrs=[col for col in X.columns if col != 'User'],\n",
    "        ordinal_ftrs=[], ordinal_cats=[], categorical_ftrs=['User']\n",
    "    )\n",
    "\n",
    "    # Record test accuracy\n",
    "    test_scores.append(test_score)\n",
    "    print(\"Best Hyperparameter and Test Accuracy: \"\n",
    "      f\"C={float(best_params['C']):.3f}, \"\n",
    "      f\"l1_ratio={float(best_params['l1_ratio']):.2f}, \"\n",
    "      f\"test accuracy={test_score:.4f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nMean test accuracy = {np.mean(test_scores):.4f}, std = {np.std(test_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe287f",
   "metadata": {},
   "source": [
    "### Problem 3b (10 points)\n",
    "\n",
    "How would you split the dataset if we wanted to know how accurately we can predict the hand postures of a new, previously unseen user? What's the target variable? Write down your reasoning (the usual 1-2 paragraphs are fine). Split the dataset into training, validation, and test sets, and run `linear_ML_pipeline` on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2517fa4",
   "metadata": {},
   "source": [
    "**Add your explanation here:**\n",
    "\n",
    "\n",
    "In this task, the goal is to evaluate how well the model can predict the hand postures of a previously unseen user. Therefore, the target variable (y) is the hand posture class 'Class'. Since we want to simulate predicting for a new user not seen during training, we must ensure that no user appears in more than one split. To achieve this, the dataset is split by 'User' rather than by individual samples, preventing data leakage across the same user’s data. We use nested GroupShuffleSplit to create approximately a 60/20/20 ratio for training, validation, and test sets. The outer split (80/20) separates users for testing, while the inner split (75/25) divides the remaining users into training and validation subsets. This approach ensures that the model is evaluated on completely unseen users, making the test accuracy a realistic estimate of the model’s ability to generalize across people.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d482903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameter and Test Accuracy: C=10.000, l1_ratio=1.00, test accuracy=0.4562\n",
      "Best Hyperparameter and Test Accuracy: C=10.000, l1_ratio=0.10, test accuracy=0.4562\n",
      "Best Hyperparameter and Test Accuracy: C=0.010, l1_ratio=0.33, test accuracy=0.4747\n",
      "Best Hyperparameter and Test Accuracy: C=0.010, l1_ratio=0.10, test accuracy=0.5167\n",
      "Best Hyperparameter and Test Accuracy: C=10.000, l1_ratio=0.10, test accuracy=0.4797\n",
      "\n",
      "Group-based split: mean test accuracy = 0.4767, std = 0.0222\n"
     ]
    }
   ],
   "source": [
    "# add your code here\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import numpy as np\n",
    "# Define X,y, group\n",
    "groups = df['User']\n",
    "X = df.drop(columns=['Class', 'User'])\n",
    "y = df['Class']\n",
    "# Create a list to store resule\n",
    "test_scores = []\n",
    "for seed in range(5):\n",
    "    # Outer split: separate test users (20%)\n",
    "    outer_gss = GroupShuffleSplit(n_splits=1, train_size=0.8, test_size=0.2, random_state=seed)\n",
    "    trainval_idx, test_idx = next(outer_gss.split(X, y, groups))\n",
    "    \n",
    "    X_trainval, X_test = X.iloc[trainval_idx], X.iloc[test_idx]\n",
    "    y_trainval, y_test = y.iloc[trainval_idx], y.iloc[test_idx]\n",
    "    groups_trainval = groups.iloc[trainval_idx]\n",
    "    \n",
    "    # Inner split: from the remaining 80%, split train/val (75/25) -> 60/20 overall\n",
    "    inner_gss = GroupShuffleSplit(n_splits=1, train_size=0.75, test_size=0.25, random_state=seed)\n",
    "    train_idx, val_idx = next(inner_gss.split(X_trainval, y_trainval, groups_trainval))\n",
    "\n",
    "    X_train, X_val = X_trainval.iloc[train_idx], X_trainval.iloc[val_idx]\n",
    "    y_train, y_val = y_trainval.iloc[train_idx], y_trainval.iloc[val_idx]\n",
    "\n",
    "    # Run pipeline\n",
    "    best_model, best_params, test_score = linear_ML_pipeline(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        is_classif=True,\n",
    "        continuous_ftrs=list(X.columns),\n",
    "        ordinal_ftrs=[], ordinal_cats=[], categorical_ftrs=[]\n",
    "    )\n",
    "\n",
    "    test_scores.append(test_score)\n",
    "    print(\"Best Hyperparameter and Test Accuracy: \"\n",
    "      f\"C={float(best_params['C']):.3f}, \"\n",
    "      f\"l1_ratio={float(best_params['l1_ratio']):.2f}, \"\n",
    "      f\"test accuracy={test_score:.4f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nGroup-based split: mean test accuracy = {np.mean(test_scores):.4f}, std = {np.std(test_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08339c1",
   "metadata": {},
   "source": [
    "### Problem 3c (10 points)\n",
    "\n",
    "How would you split the data if we wanted to identify a user based on their hand postures? What's the target variable? Follow the same steps as in 3b (explain your reasoning, split, and run `linear_ML_pipeline`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e56a2",
   "metadata": {},
   "source": [
    "**Add your explanation here:**\n",
    "\n",
    "The goal of this task is to determine which user performed a given hand posture. Therefore, the target variable (y) is 'User'. Since each user contributed a different number of samples, the dataset is highly unbalanced, some users (e.g., ID 10 and 0) have over 9,000 samples, whereas others (such as ID 4 and 7) have only a few hundred. To prevent the training and validation sets from being dominated by these majority users, we employ a stratified split. This approach maintains approximately the same user-label distribution in the training, validation, and test sets, ensuring that minority users are still represented. We first perform an 80/20 stratified split to separate the test set and then apply Stratified K-Fold (5 folds) on the remaining 80 percent to obtain training and validation sets. This way, each fold respects the user-frequency balance while allowing cross-validated evaluation of model stability. In the linear_ML_pipeline, we treat the numeric sensor variables as continuous features and the Class column as a categorical feature, enabling the model to learn how each user uniquely performs the same hand posture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce64a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameter and Test Accuracy: C=10.000, l1_ratio=0.10, test accuracy=0.2599\n",
      "Best Hyperparameter and Test Accuracy: C=10.000, l1_ratio=1.00, test accuracy=0.2616\n",
      "Best Hyperparameter and Test Accuracy: C=10.000, l1_ratio=0.10, test accuracy=0.2592\n",
      "Best Hyperparameter and Test Accuracy: C=10.000, l1_ratio=0.78, test accuracy=0.2637\n",
      "Best Hyperparameter and Test Accuracy: C=10.000, l1_ratio=0.33, test accuracy=0.2685\n",
      "\n",
      "Stratified split: mean test accuracy = 0.2626, std = 0.0033\n"
     ]
    }
   ],
   "source": [
    "# add your code here\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Define target (y) and features (X)\n",
    "X = df.drop(columns=['User'])\n",
    "y = df['User']\n",
    "test_scores = []\n",
    "\n",
    "for seed in range(5):\n",
    "    # stratified 80/20 split for test set\n",
    "    X_other, X_test, y_other, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=seed)\n",
    "\n",
    "    # StratifiedKFold on remaining 80% (to get train/val)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_other, y_other)):\n",
    "        X_train = X_other.iloc[train_index]\n",
    "        y_train = y_other.iloc[train_index]\n",
    "        X_val = X_other.iloc[val_index]\n",
    "        y_val = y_other.iloc[val_index]\n",
    "\n",
    "    # Run pipeline\n",
    "    best_model, best_params, test_score = linear_ML_pipeline(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        is_classif=True,\n",
    "        continuous_ftrs=[col for col in X.columns if col != 'Class'],\n",
    "        ordinal_ftrs=[], ordinal_cats=[], categorical_ftrs=['Class'])\n",
    "\n",
    "    test_scores.append(test_score)\n",
    "    print(\"Best Hyperparameter and Test Accuracy: \"\n",
    "          f\"C={float(best_params['C']):.3f}, \"\n",
    "          f\"l1_ratio={float(best_params['l1_ratio']):.2f}, \"\n",
    "          f\"test accuracy={test_score:.4f}\")\n",
    "\n",
    "print(f\"\\nStratified split: mean test accuracy = {np.mean(test_scores):.4f}, \"\n",
    "      f\"std = {np.std(test_scores):.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
