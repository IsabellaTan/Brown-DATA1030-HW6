{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891ee364",
   "metadata": {},
   "source": [
    "## Problem set 6\n",
    "\n",
    "## Name: [Yawen Tan]\n",
    "\n",
    "## Link to your PS6 github repo: [https://github.com/IsabellaTan/Brown-DATA1030-HW6]\n",
    "\n",
    "### Problem 0 \n",
    "\n",
    "-2 points for every missing green OK sign. \n",
    "\n",
    "Make sure you are in the DATA1030 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf77c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.10\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 2.2.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.10.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.6.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.2.3 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m xgboost version 3.0.0 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m shap version 0.47.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m polars version 1.27.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m seaborn version 0.13.2 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.10 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.10\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.10\"):\n",
    "    print(FAIL, \"Python version 3.12.10 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'numpy': \"2.2.5\", 'matplotlib': \"3.10.1\",'sklearn': \"1.6.1\", \n",
    "                'pandas': \"2.2.3\",'xgboost': \"3.0.0\", 'shap': \"0.47.2\", \n",
    "                'polars': \"1.27.1\", 'seaborn': \"0.13.2\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570da73-48b5-4cf5-9265-36619f375075",
   "metadata": {},
   "source": [
    "## Problem 1 (5 points)\n",
    "\n",
    "Write a function called `linear_ML_pipeline` which takes training, validation, test sets, and a boolean variable called `is_classif` as input. The variable `is_classif` is `True` if the target variable is categorial (classification problem), and `False` if the target variable is continuous (regression problem). The function also takes the following lists as inputs:\n",
    "- continuous_ftrs: the column names of continuous features\n",
    "- ordinal_ftrs: the column names of ordinal features\n",
    "- ordinal_cats: the ordered list of categories for each ordinal feature\n",
    "- categorical_ftrs: the column names of categorical features\n",
    "\n",
    "Within the function, perform the following steps:\n",
    "- write the docstring\n",
    "- test the inputs! write at least 10 tests. make sure that among other things, all features are accounted for in the lists.\n",
    "- preprocess the sets using sklearn and make sure to fit_transform the training set, and transform the validation and test sets\n",
    "- fit a logistic regression model if `is_classif` is `True`, a linear regression model otherwise\n",
    "- use the elastic net regularization and tune both hyperparameters (alpha or C and l1_ratio)\n",
    "- perform CV and select the hyperparameter combo which optimizes the validation score\n",
    "- calculate the test score\n",
    "- your function should return the best model, its hyperparameters, and the test score\n",
    "\n",
    "You will use this function to solve problems 2 and 3. Data splitting will be performed before the function is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4436f7c-0221-4456-81bb-2a2b5f47de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n",
    "\n",
    "def linear_ML_pipeline(X_train, y_train, X_val, y_val, X_test, y_test, is_classif, continuous_ftrs, ordinal_ftrs, ordinal_cats, categorical_ftrs, random_state=42):\n",
    "\n",
    "    '''\n",
    "    Build and evaluate a complete linear-model pipeline that supports both\n",
    "    classification (Logistic Regression) and regression (Elastic Net),\n",
    "    with preprocessing, elastic-net regularization, manual hyperparameter tuning\n",
    "    over a grid, validation-based model selection, and final test evaluation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas.DataFrame\n",
    "        Feature matrix for the training split. Columns must match X_val and X_test\n",
    "        exactly (same names and order).\n",
    "    y_train : pandas.Series or 1D array-like\n",
    "        Target vector for the training split.\n",
    "    X_val : pandas.DataFrame\n",
    "        Feature matrix for the validation split (used to select hyperparameters).\n",
    "    y_val : pandas.Series or 1D array-like\n",
    "        Target vector for the validation split.\n",
    "    X_test : pandas.DataFrame\n",
    "        Feature matrix for the test split (held out for final evaluation only).\n",
    "    y_test : pandas.Series or 1D array-like\n",
    "        Target vector for the test split.\n",
    "    is_classif : bool\n",
    "        If True, fit a Logistic Regression model (elastic net penalty) for a\n",
    "        classification task; if False, fit an ElasticNet regressor for a\n",
    "        regression task.\n",
    "    continuous_ftrs : list[str]\n",
    "        Column names (in X_*) that are continuous features to be standardized\n",
    "        (mean=0, std=1).\n",
    "    ordinal_ftrs : list[str]\n",
    "        Column names (in X_*) that are ordinal categorical features to be\n",
    "        encoded via OrdinalEncoder using the specified order in `ordinal_cats`.\n",
    "    ordinal_cats : list[list[Any]]\n",
    "        Ordered categories for each ordinal feature. The i-th list provides the\n",
    "        category order for ordinal_ftrs[i].\n",
    "    categorical_ftrs : list[str]\n",
    "        Column names (in X_*) that are nominal (unordered) categorical features\n",
    "        to be encoded via OneHotEncoder(handle_unknown='ignore').\n",
    "    random_state : int, default=42\n",
    "        Random seed used where applicable (e.g., solvers), to ensure reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    best_model : sklearn estimator (already fitted)\n",
    "        The model refit on TRAIN+VAL using the best hyperparameters discovered\n",
    "        on the validation set (classification: LogisticRegression with penalty='elasticnet'\n",
    "        and solver='saga'; regression: ElasticNet).\n",
    "    best_hyperparams : dict\n",
    "        The best hyperparameter combination found on the validation set. Includes\n",
    "        at least {'l1_ratio': ..., 'C': ...} for classification or\n",
    "        {'l1_ratio': ..., 'alpha': ...} for regression, and any other key settings\n",
    "        needed to reproduce the best model.\n",
    "    test_score : float\n",
    "        Final performance on the held-out test set. For classification, accuracy;\n",
    "        for regression, R^2. (These defaults follow the introductory course\n",
    "        conventions.)\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> # Suppose you already split your data and defined feature groups:\n",
    "    >>> best_model, best_params, test_score = linear_ML_pipeline(\n",
    "    ...     X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    ...     is_classif=True,\n",
    "    ...     continuous_ftrs=['age','capital_gain','capital_loss','hours_per_week'],\n",
    "    ...     ordinal_ftrs=['education'],\n",
    "    ...     ordinal_cats=[[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',\n",
    "    ...                    ' 10th',' 11th',' 12th',' HS-grad',' Some-college',\n",
    "    ...                    ' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',\n",
    "    ...                    ' Prof-school',' Doctorate']],\n",
    "    ...     categorical_ftrs=['workclass','marital-status','occupation',\n",
    "    ...                       'relationship','race','sex','native-country'],\n",
    "    ...     random_state=42\n",
    "    ... )\n",
    "    >>> best_params\n",
    "    {'penalty': 'elasticnet', 'solver': 'saga', 'C': 1.0, 'l1_ratio': 0.5, 'max_iter': 10000}\n",
    "    >>> test_score\n",
    "    0.85  # accuracy on the held-out test set (example)\n",
    "    '''\n",
    "    # import library\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "    from sklearn.model_selection import ParameterGrid\n",
    "    from sklearn.metrics import accuracy_score, r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "    # TEST\n",
    "    #----------------------------------------------------------------\n",
    "    # Test 1: if X_train, X_val, X_test are not pandas DataFrames, raise ValueError\n",
    "    if not all(isinstance(x, pd.DataFrame) for x in [X_train, X_val, X_test]):\n",
    "        raise ValueError(\"X_train, X_val, and X_test must all be pandas DataFrames\")\n",
    "\n",
    "    # Test 2: if y_train, y_val, y_test are not pandas Series, raise ValueError\n",
    "    if not all(isinstance(y, (pd.Series, np.ndarray, list)) for y in [y_train, y_val, y_test]):\n",
    "        raise ValueError(\"y_train, y_val, and y_test must each be a pandas Series, numpy array, or list\")\n",
    "\n",
    "    # Test 3: if any of the feature DataFrames are empty (0 rows or 0 columns), raise ValueError\n",
    "    for name, df in zip([\"X_train\", \"X_val\", \"X_test\"], [X_train, X_val, X_test]):\n",
    "        if df.shape[0] == 0:\n",
    "            raise ValueError(f\"{name} has 0 rows\")\n",
    "        if df.shape[1] == 0:\n",
    "            raise ValueError(f\"{name} has 0 columns\")\n",
    "\n",
    "    # Test 4: ensure column names and order match across X_train, X_val, X_test\n",
    "    if list(X_train.columns) != list(X_val.columns) or list(X_train.columns) != list(X_test.columns):\n",
    "        raise ValueError(\"Column names and order must be identical across X_train, X_val, and X_test\")\n",
    "\n",
    "    # Test 5: ensure feature lists are all Python lists\n",
    "    for var_name, var in {\n",
    "        \"continuous_ftrs\": continuous_ftrs,\n",
    "        \"ordinal_ftrs\": ordinal_ftrs,\n",
    "        \"ordinal_cats\": ordinal_cats,\n",
    "        \"categorical_ftrs\": categorical_ftrs,\n",
    "    }.items():\n",
    "        if not isinstance(var, list):\n",
    "            raise ValueError(f\"{var_name} must be provided as a list\")\n",
    "\n",
    "    # Test 6: check that every feature column is accounted for exactly once\n",
    "    all_features = continuous_ftrs + ordinal_ftrs + categorical_ftrs\n",
    "    if set(all_features) != set(X_train.columns):\n",
    "        missing = set(X_train.columns) - set(all_features)\n",
    "        extra = set(all_features) - set(X_train.columns)\n",
    "        raise ValueError(f\"Feature mismatch: missing {missing}, extra {extra}\")\n",
    "\n",
    "    # Test 7: check for overlaps among feature groups\n",
    "    if (set(continuous_ftrs) & set(ordinal_ftrs)) or \\\n",
    "       (set(continuous_ftrs) & set(categorical_ftrs)) or \\\n",
    "       (set(ordinal_ftrs) & set(categorical_ftrs)):\n",
    "        raise ValueError(\"continuous_ftrs, ordinal_ftrs, and categorical_ftrs must not overlap\")\n",
    "\n",
    "    # Test 8: ensure ordinal_ftrs and ordinal_cats have the same length\n",
    "    if len(ordinal_ftrs) != len(ordinal_cats):\n",
    "        raise ValueError(\"Length of ordinal_cats must match the number of ordinal_ftrs\")\n",
    "\n",
    "    # Test 9: ensure continuous_ftrs is not empty\n",
    "    if continuous_ftrs == [] and ordinal_ftrs == [] and categorical_ftrs == []:\n",
    "        raise ValueError(\"At least one feature list must be non-empty\")\n",
    "\n",
    "    # Test 10: ensure is_classif is a boolean\n",
    "    if not isinstance(is_classif, bool):\n",
    "        raise ValueError(\"is_classif must be a boolean value (True for classification, False for regression)\")\n",
    "\n",
    "    # Test 11: ensure target lengths match feature lengths\n",
    "    for (x, y, name) in [(X_train, y_train, \"train\"), (X_val, y_val, \"validation\"), (X_test, y_test, \"test\")]:\n",
    "        if len(x) != len(y):\n",
    "            raise ValueError(f\"Length mismatch between X_{name} and y_{name}\")\n",
    "\n",
    "    # Test 12: optional—check for duplicate columns in X_train\n",
    "    if X_train.columns.duplicated().any():\n",
    "        raise ValueError(\"Duplicate column names detected in X_train\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Data Preprocessing\n",
    "    #----------------------------------------------------------------\n",
    "    # Continuous features: StandardScaler\n",
    "    continuous_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "    # Ordinal features: OrdinalEncoder\n",
    "    ordinal_transformer = Pipeline(steps=[('encoder', OrdinalEncoder(categories=ordinal_cats))])\n",
    "\n",
    "    # Nominal (categorical) features: OneHotEncoder\n",
    "    categorical_transformer = Pipeline(steps=[('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "    # Combine all three transformers using ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cont', continuous_transformer, continuous_ftrs),\n",
    "            ('ord', ordinal_transformer, ordinal_ftrs),\n",
    "            ('cat', categorical_transformer, categorical_ftrs)],\n",
    "        remainder='drop'  # if feature list cover all the column, drop left feature\n",
    "        )\n",
    "\n",
    "    # Fit on training data, transform all sets\n",
    "    X_train_prep = preprocessor.fit_transform(X_train)\n",
    "    X_val_prep = preprocessor.transform(X_val)\n",
    "    X_test_prep = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    #  Model setup and Elastic Net regularization\n",
    "    #----------------------------------------------------------------\n",
    "    from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "    from sklearn.metrics import accuracy_score, r2_score, mean_squared_error\n",
    "    from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "    if is_classif:\n",
    "        # Logistic Regression + Elastic Net Regularization\n",
    "        model_type = \"classification\"\n",
    "        base_model = LogisticRegression(\n",
    "            penalty=\"elasticnet\",\n",
    "            solver=\"saga\",\n",
    "            max_iter=3000,\n",
    "            random_state=random_state)\n",
    "\n",
    "        # Parameter tuning\n",
    "        param_grid = {\n",
    "            \"C\": np.logspace(-4, 4, 5),       # [1e-4, 1e-2, 1, 1e2, 1e4]\n",
    "            \"l1_ratio\": np.linspace(0.1, 1.0, 5)}\n",
    "        score_func = accuracy_score\n",
    "        score_name = \"Accuracy\"\n",
    "\n",
    "    else:\n",
    "        # Linear Regression：ElasticNet\n",
    "        model_type = \"regression\"\n",
    "        base_model = ElasticNet(\n",
    "            max_iter=5000,\n",
    "            random_state=random_state)\n",
    "\n",
    "        # Parameter tuning\n",
    "        param_grid = {\n",
    "            \"alpha\": np.logspace(-4, 1, 6),   # [1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "            \"l1_ratio\": np.linspace(0.1, 1.0, 6)}\n",
    "\n",
    "        score_func = r2_score # use R square\n",
    "        mse_func = mean_squared_error\n",
    "        score_name = \"R²\"\n",
    "\n",
    "    # Cross-validation, hyperparameter tuning, and model selection\n",
    "    # ----------------------------------------------------------------\n",
    "    best_val_score = -np.inf  # record the best score for validation set\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    # set a list to store result\n",
    "    results = []\n",
    "\n",
    "    # Loop every combinition of parameter\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        # Create new model for new parameter\n",
    "        model = base_model.set_params(**params)\n",
    "        # Fit the train data\n",
    "        model.fit(X_train_prep, y_train)\n",
    "        # Predit on train and validation data\n",
    "        y_train_pred = model.predict(X_train_prep)\n",
    "        y_val_pred = model.predict(X_val_prep)\n",
    "        # Calcuate the score\n",
    "        train_score = score_func(y_train, y_train_pred)\n",
    "        val_score = score_func(y_val, y_val_pred)\n",
    "        mse_val = None\n",
    "        if not is_classif:\n",
    "            mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "        # Store the result\n",
    "        results.append({\n",
    "            **params,\n",
    "            \"train_score\": train_score,\n",
    "            \"val_score\": val_score,\n",
    "            \"mse_val\": mse_val\n",
    "        })\n",
    "        # Update best model\n",
    "        if val_score > best_val_score:\n",
    "            best_val_score = val_score\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "\n",
    "    # Refit best model on TRAIN+VAL, evaluate on TEST\n",
    "    # ---------------------------------------------------------\n",
    "    # Combine train and validation set\n",
    "    X_trainval = np.vstack([X_train_prep, X_val_prep])\n",
    "    y_trainval = np.concatenate([y_train, y_val])\n",
    "    # use best parameter to train the model\n",
    "    final_model = base_model.set_params(**best_params)\n",
    "    final_model.fit(X_trainval, y_trainval)\n",
    "    # predict on the test set and calculate the score\n",
    "    y_test_pred = final_model.predict(X_test_prep)\n",
    "    test_score = score_func(y_test, y_test_pred)\n",
    "\n",
    "    # For regression: use R² as the final evaluation metric (consistent with lecture and default sklearn scoring)\n",
    "    return final_model, best_params, test_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706733a7-d4ca-43bb-9dd0-d48bf78debfc",
   "metadata": {},
   "source": [
    "## Problem 2: time series forecasting with VAR\n",
    "\n",
    "You will practice multivariate time series forcasting using VAR - vector autoregression.\n",
    "\n",
    "The `stocks_prices.csv` is in the data folder. It contains the stock prices of amazon (AMZN), microsoft (MSFT), and apple (AAPL). Here is a description of each column in the dataset:\n",
    "- price ticker date: the date when the stock price was recorded - note that weekends and holidays are absent\n",
    "- Close AAPL: apple stock price at closing time in USD (i.e., at the end of the trading day)\n",
    "- Close AMZN: amazon stock price at closing time in USD (i.e., at the end of the trading day)\n",
    "- Close MSFT: microsoft stock price at closing time in USD (i.e., at the end of the trading day)\n",
    "- High AAPL: highest apple stock price during the trading day in USD\n",
    "- High AMZN: highest amazon stock price during the trading day in USD\n",
    "- High MSFT: highest microsoft stock price during the trading day in USD\n",
    "- Low AAPL: lowest apple stock price during the trading day in USD\n",
    "- Low AMZN: lowest amazon stock price during the trading day in USD\n",
    "- Low MSFT: lowest microsoft stock price during the trading day in USD\n",
    "- Open AAPL: apple stock price at opening time in USD (i.e., at the beginning of the trading day)\n",
    "- Open AMZN: amazon stock price at opening time in USD (i.e., at the beginning of the trading day)\n",
    "- Open MSFT: microsoft stock price opening time in USD (i.e., at the beginning of the trading day)\n",
    "- Volume AAPL: total traded volume (buys and sells) of apple on the trading day in USD\n",
    "- Volume AMZN: total traded volume (buys and sells) of amazon on the trading day in USD\n",
    "- Volume MSFT: total traded volume (buys and sells) of microsoft on the trading day in USD\n",
    "\n",
    "The goal of problem 2 is to predict the opening price of apple stocks one day ahead based on the time series observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00182271-de3b-4ed6-8e66-df74c4500aae",
   "metadata": {},
   "source": [
    "### Problem 2a - feature matrix (15 points)\n",
    "Perform the steps outlined in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49769466-6ab3-4c6b-a8e1-58e5dc9bd17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n",
    "# import packages\n",
    "\n",
    "\n",
    "# read in the dataset and print the header\n",
    "\n",
    "\n",
    "# write a function which takes the following input:\n",
    "# - the dataframe, \n",
    "# - the name of the target column, \n",
    "# - a variable `p` which describes how many autoregresive past features we use (p in AR(p) of the lecture notes)\n",
    "# the function should return a feature matrix X and target variable y after VAR is applied to the multivariate time series data\n",
    "# make sure that the points are ordered with respect to time such that \n",
    "# the oldest observations are at the top and the most recent observation are at the bottom of the dataframe.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b38e13-f080-4d49-a1d8-22d9efb7efe6",
   "metadata": {},
   "source": [
    "### Problem 2b - splitting (10 points)\n",
    "We will split the dataset in a couple of different ways to study information leakage. Perform the steps outlined in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d49515-ebb5-47dd-bb27-2019e766b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n",
    "\n",
    "# split 1: create shuffled train/validation/test sets (60/20/20 ratio) and run `linear_ML_pipeline` on it. \n",
    "# print out the best hyperparameter values, and the test score\n",
    "# repeat this process with 5 random states\n",
    "# print out the best hyperparameter values, and the test score\n",
    "# print out the mean and stdev of the 5 test scores.\n",
    "\n",
    "\n",
    "# split 2: place 20% of the most recent observations in the test set, \n",
    "# then apply sklearn's TimeSeriesSplit on the rest of the data with n_splits = 5\n",
    "# run `linear_ML_pipeline` on each split.\n",
    "# print out the best hyperparameter values, and the test score\n",
    "# print out the mean and stdev of the 5 test scores.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7488498-2914-448d-a69e-43b166389eeb",
   "metadata": {},
   "source": [
    "Discuss in this cell what you observe and answer the questions below:\n",
    "\n",
    "- the mean and stdev of the test scores differ based on how you split the data. Explain in a few sentences why!\n",
    "\n",
    "your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a51215-6434-4aa6-906a-57436b3000c9",
   "metadata": {},
   "source": [
    "- Forecasting models like this can be used for trading. I.e., if your model predicts that the opening price of the apple stock tomorrow will be higher than the closing price today, you'd put in a sell order at the end of the day. If your prediction is correct, you will make a profit in USD once your order executes at tomorrow's open. Similarly, if your model predicts that the opening price of apple stock tomorrow will be lower than the closing price today, you'd put in a buy order. If your prediction is correct, you'll buy at a low price once your order executes at tomorrow's open. This is how you'd act based on the model's prediction - buy low, sell high. Would you be willing to use your own money to deploy the models developed in split 1 and/or split 2? Why or why not?\n",
    "\n",
    "your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded0881",
   "metadata": {},
   "source": [
    "## Problem 3 - group structure\n",
    "\n",
    "We will work with the [hand postures dataset](https://archive.ics.uci.edu/ml/datasets/Motion+Capture+Hand+Postures) in problem 3. Please carefully read the dataset description and perform as much EDA as you can on this dataset. The EDA is not graded but it will help you to correctly answer 3a and 3b.\n",
    "\n",
    "This dataset has group structure: 14 users performing 5 different hand postures while wearing sensors attached to a left-handed glove. Two different ML questions can be asked using this dataset. We will explore how splitting differs for both questions in 2a and 2b.\n",
    "\n",
    "Later on, we'll teach you how to deal with missing data. For now, simply drop all columns with any missing values (**don't do this in real life**, but for now it's fine). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6443c3ab-843d-429f-8957-a1f8476601ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d54c4-f02c-4c4a-9756-19d25c899209",
   "metadata": {},
   "source": [
    "## Problem 3a - basic splitting (5 points)\n",
    "\n",
    "Create shuffled train/validation/test sets (60/20/20 ratio) and use `linear_ML_pipeline` to predict the class. \n",
    "Print out the best hyperparameter values, and the test score.\n",
    "Repeat this process with 5 random states and report the mean and stdev of the test score.\n",
    "\n",
    "You may receive some warnings about models failing to converge. This usually happens when C is too high (aka not enough regularization). Play around with those parameters in `linear_ML_pipeline` until you no longer see that warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963af1c-4cdd-49d4-8347-faff8cb7d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe287f",
   "metadata": {},
   "source": [
    "### Problem 3b (10 points)\n",
    "\n",
    "How would you split the dataset if we wanted to know how accurately we can predict the hand postures of a new, previously unseen user? What's the target variable? Write down your reasoning (the usual 1-2 paragraphs are fine). Split the dataset into training, validation, and test sets, and run `linear_ML_pipeline` on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2517fa4",
   "metadata": {},
   "source": [
    "Add your explanation here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d482903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08339c1",
   "metadata": {},
   "source": [
    "### Problem 3c (10 points)\n",
    "\n",
    "How would you split the data if we wanted to identify a user based on their hand postures? What's the target variable? Follow the same steps as in 3b (explain your reasoning, split, and run `linear_ML_pipeline`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e56a2",
   "metadata": {},
   "source": [
    "Add your explanation here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce64a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
